1
00:02:59,400 --> 00:03:03,370
Okay, so mp3, you

2
00:03:03,370 --> 00:03:11,964
do three algorithms, right?

3
00:03:11,964 --> 00:03:17,300
Monte-carlo,
something like this.

4
00:03:18,960 --> 00:03:21,504
You start somewhere.

5
00:03:21,504 --> 00:03:29,110
Some poll. A trajectory
starting from,

6
00:03:29,110 --> 00:03:35,059
say sum is zero and goes to
as T. And in there are some,

7
00:03:35,059 --> 00:03:39,480
Everything is sum
as i for some i.

8
00:03:41,560 --> 00:03:45,859
And then for each state,

9
00:03:45,859 --> 00:03:49,325
look at the trajectory,

10
00:03:49,325 --> 00:03:51,110
the part of the
trajectory that goes from

11
00:03:51,110 --> 00:03:53,629
that state to the
terminal state.

12
00:03:53,629 --> 00:03:55,820
And understand that, that is

13
00:03:55,820 --> 00:03:58,730
one sample of the
possible trajectories

14
00:03:58,730 --> 00:04:01,460
that you can think of that can

15
00:04:01,460 --> 00:04:04,714
possibly happen going out
from that particular state.

16
00:04:04,714 --> 00:04:08,670
So you calculate

17
00:04:10,390 --> 00:04:14,039
the discounted

18
00:04:23,880 --> 00:04:26,755
somehow rewards.

19
00:04:26,755 --> 00:04:29,260
Is this confusing?

20
00:04:29,260 --> 00:04:30,115
Okay?

21
00:04:30,115 --> 00:04:34,434
This is offsetting
this I to zero, right?

22
00:04:34,434 --> 00:04:38,379
And you count from there
to the termination.

23
00:04:38,379 --> 00:04:43,495
And then for each
of these states,

24
00:04:43,495 --> 00:04:50,410
collect a bunch of
these g's, right?

25
00:04:50,410 --> 00:04:51,729
And then in the end,

26
00:04:51,729 --> 00:04:55,660
when we ask for the value there,

27
00:04:55,660 --> 00:04:58,184
the Pi given to you as just a,

28
00:04:58,184 --> 00:04:59,945
the average of that,

29
00:04:59,945 --> 00:05:01,444
which is the sound of g

30
00:05:01,444 --> 00:05:04,650
divided by the
number, the average.

31
00:05:05,740 --> 00:05:08,945
And the second algorithm,

32
00:05:08,945 --> 00:05:10,910
hopefully you fully understand

33
00:05:10,910 --> 00:05:14,149
now is saying it doesn't matter

34
00:05:14,149 --> 00:05:17,299
where your status just

35
00:05:17,299 --> 00:05:19,624
go with the flow
going into the game.

36
00:05:19,624 --> 00:05:24,929
Whenever you take one
transition from S to S prime,

37
00:05:25,990 --> 00:05:29,404
update the value estimates.

38
00:05:29,404 --> 00:05:32,405
Your beginning state.

39
00:05:32,405 --> 00:05:35,390
The first state in
the pair of states.

40
00:05:35,390 --> 00:05:37,739
Using.

41
00:05:38,740 --> 00:05:43,505
The update formula
that is looking at

42
00:05:43,505 --> 00:05:49,699
the difference between your
one sample of Bellman update.

43
00:05:49,699 --> 00:05:53,015
Again, your true Bellman update
should be thinking about

44
00:05:53,015 --> 00:05:54,919
all the possible
things that can happen

45
00:05:54,919 --> 00:05:57,274
with some probabilities, right?

46
00:05:57,274 --> 00:05:59,840
But now you see there's
one sample which is like

47
00:05:59,840 --> 00:06:03,499
the x k were talking about
was the random variable.

48
00:06:03,499 --> 00:06:07,100
So what do you want to do is do

49
00:06:07,100 --> 00:06:11,390
things according to something

50
00:06:11,390 --> 00:06:14,690
roughly similar to
the Bellman update

51
00:06:14,690 --> 00:06:16,580
in which you think about

52
00:06:16,580 --> 00:06:24,769
this one sample of

53
00:06:24,769 --> 00:06:27,004
going from this state to

54
00:06:27,004 --> 00:06:31,099
attain the estimated
value at the next state.

55
00:06:31,099 --> 00:06:34,894
Again, this is one
possible outcome

56
00:06:34,894 --> 00:06:36,589
of all the possible
things that may happen

57
00:06:36,589 --> 00:06:39,230
here with a temporary guess

58
00:06:39,230 --> 00:06:41,629
of the value on the next state.

59
00:06:41,629 --> 00:06:42,470
Alright?

60
00:06:42,470 --> 00:06:45,410
And then you look at the
difference that this,

61
00:06:45,410 --> 00:06:49,550
if this is going to be
happening 100% of time,

62
00:06:49,550 --> 00:06:53,944
then you should just set your
true value to this, right?

63
00:06:53,944 --> 00:06:55,715
But we know that
is not the case.

64
00:06:55,715 --> 00:06:59,509
We need to discount it somehow

65
00:06:59,509 --> 00:07:01,204
was the knowledge about

66
00:07:01,204 --> 00:07:04,610
how many times we have seen
this particular state.

67
00:07:04,610 --> 00:07:05,900
Right?

68
00:07:05,900 --> 00:07:09,245
So you look at this
one sample point,

69
00:07:09,245 --> 00:07:10,960
look at the difference was

70
00:07:10,960 --> 00:07:14,704
your current understanding
about the value of

71
00:07:14,704 --> 00:07:21,930
your S and then multiply
with some learning rate,

72
00:07:22,000 --> 00:07:25,205
which is dependent on,

73
00:07:25,205 --> 00:07:29,435
say, alpha, dependent on S.

74
00:07:29,435 --> 00:07:32,285
So this is our,

75
00:07:32,285 --> 00:07:36,120
say, the number of visits to S.

76
00:07:36,160 --> 00:07:37,565
Alright?

77
00:07:37,565 --> 00:07:39,064
So in the code,

78
00:07:39,064 --> 00:07:40,700
you should see that clearly.

79
00:07:40,700 --> 00:07:45,410
So this is something that in
principle, we should say.

80
00:07:45,410 --> 00:07:50,480
If this is just trying to
get one static average,

81
00:07:50,480 --> 00:07:57,089
it's potentially just the
number of visits to that state.

82
00:07:58,270 --> 00:08:00,634
But in general, this can be

83
00:08:00,634 --> 00:08:02,389
just something that's
less than one.

84
00:08:02,389 --> 00:08:04,579
And it allows you to
tune the speed of

85
00:08:04,579 --> 00:08:08,059
convergence by calling it

86
00:08:08,059 --> 00:08:11,000
some sort of step size
or learning rate.

87
00:08:11,000 --> 00:08:11,900
Alright?

88
00:08:11,900 --> 00:08:13,279
I think we have given

89
00:08:13,279 --> 00:08:17,300
that precise number
to you in the PA.

90
00:08:17,300 --> 00:08:20,665
So this is the increment

91
00:08:20,665 --> 00:08:24,235
you want to make to
the previous estimate.

92
00:08:24,235 --> 00:08:26,305
Everything has a pi here.

93
00:08:26,305 --> 00:08:28,810
By looking at this
one data point

94
00:08:28,810 --> 00:08:30,669
and the difference was
your current guests.

95
00:08:30,669 --> 00:08:33,100
So this is the
change you want to

96
00:08:33,100 --> 00:08:37,819
make to your previous estimate.

97
00:08:39,900 --> 00:08:41,349
Alright?

98
00:08:41,349 --> 00:08:44,899
So everything here that

99
00:08:45,240 --> 00:08:48,205
we talked about in
these two algorithms

100
00:08:48,205 --> 00:08:51,019
follows the given pi.

101
00:08:52,050 --> 00:08:54,505
When you need to take a step,

102
00:08:54,505 --> 00:08:57,519
you will follow the action
that's given to you by the Pi.

103
00:08:57,519 --> 00:09:01,759
And then we started
talking about Q-learning,

104
00:09:06,700 --> 00:09:10,820
which tries to give you

105
00:09:10,820 --> 00:09:14,390
an algorithm that allows you
to just go into the game,

106
00:09:14,390 --> 00:09:15,530
go into the world,

107
00:09:15,530 --> 00:09:21,139
the MDP, some state and
started doing things.

108
00:09:21,139 --> 00:09:22,385
Okay?

109
00:09:22,385 --> 00:09:25,010
Then you need to
answer two questions.

110
00:09:25,010 --> 00:09:32,180
One is not addressed by
our previous lecture yet.

111
00:09:32,180 --> 00:09:35,820
What is the action
that I should take?

112
00:09:36,220 --> 00:09:40,649
Okay, so operationally,
what should you do?

113
00:09:41,590 --> 00:09:43,859
Yeah.

114
00:09:47,020 --> 00:09:49,579
You don't need to answer
yet, but I don't have

115
00:09:49,579 --> 00:09:53,525
a policy here now.

116
00:09:53,525 --> 00:09:55,849
So Q-learning, so no pi now,

117
00:09:55,849 --> 00:09:58,519
this is our new question.

118
00:09:58,519 --> 00:10:00,994
So we want to just go into,

119
00:10:00,994 --> 00:10:04,549
because no one gave us
others about what to do.

120
00:10:04,549 --> 00:10:06,710
And let's come up with
an algorithm that

121
00:10:06,710 --> 00:10:09,034
is figures out the
best thing to do that.

122
00:10:09,034 --> 00:10:10,579
I'll feel lousy.
Yeah. So you don't

123
00:10:10,579 --> 00:10:13,109
start with a policy anymore.

124
00:10:14,980 --> 00:10:17,249
Okay.

125
00:10:17,350 --> 00:10:20,254
So we need to answer
this question.

126
00:10:20,254 --> 00:10:24,994
And then we partially
started talking about,

127
00:10:24,994 --> 00:10:26,750
in this case, what should be

128
00:10:26,750 --> 00:10:29,720
the value that you
want to keep track of?

129
00:10:29,720 --> 00:10:31,234
The difference?

130
00:10:31,234 --> 00:10:33,845
Again, is when we
have a fixed pie,

131
00:10:33,845 --> 00:10:37,069
we can say a state.

132
00:10:37,069 --> 00:10:40,865
You follow the order

133
00:10:40,865 --> 00:10:45,769
of policy given to you and
it branches out, blah, blah.

134
00:10:45,769 --> 00:10:51,320
And now we don't have
a policy anymore.

135
00:10:51,320 --> 00:10:55,019
We have a bunch of
possible choices.

136
00:10:56,280 --> 00:10:59,080
Alright, each one corresponds to

137
00:10:59,080 --> 00:11:01,795
an action that I
can possibly take.

138
00:11:01,795 --> 00:11:04,449
And then we're after you

139
00:11:04,449 --> 00:11:06,984
decide on taking any
particular actions,

140
00:11:06,984 --> 00:11:12,370
the future is potentially
very different

141
00:11:12,370 --> 00:11:19,159
with Neil probabilities
that depend on the action.

142
00:11:19,710 --> 00:11:21,970
And we were saying,
for instance,

143
00:11:21,970 --> 00:11:31,009
think about the state of
having two cards of ten.

144
00:11:32,430 --> 00:11:35,319
If I ask you,

145
00:11:35,319 --> 00:11:39,019
what is the value of state?

146
00:11:40,140 --> 00:11:47,589
For that state, then it's
pretty hard to answer, right?

147
00:11:47,589 --> 00:11:50,994
Because whether
it's a good state

148
00:11:50,994 --> 00:11:55,284
or a bad state depends crucially
on what you actually do.

149
00:11:55,284 --> 00:11:57,520
That state, if you take
the bad action and

150
00:11:57,520 --> 00:11:59,485
it becomes a terrible
state, alright?

151
00:11:59,485 --> 00:12:00,955
If you take the good action,

152
00:12:00,955 --> 00:12:03,955
that is the state that you
always will be happy to see.

153
00:12:03,955 --> 00:12:06,594
You'll want to bet more
money on that state.

154
00:12:06,594 --> 00:12:09,745
Alright, so because of that,

155
00:12:09,745 --> 00:12:14,859
we realized that in
the case of not having

156
00:12:14,859 --> 00:12:17,109
a policy and you need to form

157
00:12:17,109 --> 00:12:21,055
your own understanding
about actions that states.

158
00:12:21,055 --> 00:12:23,659
We should start thinking about

159
00:12:23,659 --> 00:12:26,660
the values of state
and action pairs.

160
00:12:26,660 --> 00:12:28,279
That's what we call q-value,

161
00:12:28,279 --> 00:12:30,124
is no longer a state value,

162
00:12:30,124 --> 00:12:32,879
but it's a state action that

163
00:12:36,640 --> 00:12:41,374
then we want to do
this in the same way

164
00:12:41,374 --> 00:12:45,574
as what Bellman
equation told us.

165
00:12:45,574 --> 00:12:47,750
Because we know if we
do things according

166
00:12:47,750 --> 00:12:49,940
to Bellman equation and
everything is contraction,

167
00:12:49,940 --> 00:12:52,385
nice and converging
and all that, right?

168
00:12:52,385 --> 00:12:55,415
So we should try our
best to make sure even

169
00:12:55,415 --> 00:12:59,119
if we shift our focus to this
state action pair of thing,

170
00:12:59,119 --> 00:13:01,129
we shouldn't deviate from what

171
00:13:01,129 --> 00:13:04,505
Bellman equation has
formulated for us.

172
00:13:04,505 --> 00:13:05,659
Alright?

173
00:13:05,659 --> 00:13:09,964
Now, the option for
that, one suggestion,

174
00:13:09,964 --> 00:13:12,425
one design for doing

175
00:13:12,425 --> 00:13:16,669
value estimates on state action
pairs now is to think of

176
00:13:16,669 --> 00:13:27,329
state values as the best
Q value that you can get.

177
00:13:27,490 --> 00:13:29,105
Right?

178
00:13:29,105 --> 00:13:36,575
So you want to do that again.

179
00:13:36,575 --> 00:13:39,559
This is exactly what
you did in 2048, right?

180
00:13:39,559 --> 00:13:42,769
So you are, suppose
you have computers

181
00:13:42,769 --> 00:13:44,149
and members for those and you're

182
00:13:44,149 --> 00:13:45,980
wondering about what
actions to take here,

183
00:13:45,980 --> 00:13:48,710
just look at those
numbers and take the max.

184
00:13:48,710 --> 00:13:49,594
Then max.

185
00:13:49,594 --> 00:13:51,334
If you annotate the root node or

186
00:13:51,334 --> 00:13:54,050
the intermediate max player's
node should be the number

187
00:13:54,050 --> 00:13:58,624
you backpropagate to
the upper level, right?

188
00:13:58,624 --> 00:14:01,100
This is doing exactly that.

189
00:14:01,100 --> 00:14:03,269
Okay?

190
00:14:03,340 --> 00:14:05,630
And then all we want to do,

191
00:14:05,630 --> 00:14:08,014
so this is the definition of

192
00:14:08,014 --> 00:14:10,190
Q values that we want to use to

193
00:14:10,190 --> 00:14:12,455
keep track of the
quality of actions.

194
00:14:12,455 --> 00:14:14,030
And then we want to make sure

195
00:14:14,030 --> 00:14:15,769
that we are going to
write down something

196
00:14:15,769 --> 00:14:17,899
that's consistent with the
Bellman equation because

197
00:14:17,899 --> 00:14:20,300
that's the best
thing ever, right?

198
00:14:20,300 --> 00:14:24,305
So we know the value is,

199
00:14:24,305 --> 00:14:27,329
according to Bell men.

200
00:14:47,070 --> 00:14:50,244
Yeah, the expectation value

201
00:14:50,244 --> 00:14:53,870
of next states given as an aid.

202
00:14:59,970 --> 00:15:04,344
Then, well we can do is push

203
00:15:04,344 --> 00:15:07,870
everything in to the max
operator and expectation.

204
00:15:07,870 --> 00:15:10,390
Because if you're
maximizing something,

205
00:15:10,390 --> 00:15:12,999
you can maximize it up to

206
00:15:12,999 --> 00:15:16,240
some constant factor and an
additional constant term.

207
00:15:16,240 --> 00:15:19,420
So nothing in here

208
00:15:19,420 --> 00:15:23,124
is dependent on this
number or this number.

209
00:15:23,124 --> 00:15:27,729
So I can say maximize a

210
00:15:27,729 --> 00:15:30,949
in S. And then

211
00:15:30,949 --> 00:15:33,740
same thing can be said
about the expectation.

212
00:15:33,740 --> 00:15:36,694
I can push everything into
the expectation, right?

213
00:15:36,694 --> 00:15:43,114
So all I want to do
for each state is to

214
00:15:43,114 --> 00:15:51,300
maximize the expectation
of given us an a.

215
00:15:54,910 --> 00:15:57,240
Okay?

216
00:15:57,550 --> 00:16:01,650
So now you see this

217
00:16:06,040 --> 00:16:13,409
becomes what I can
define as the q-value.

218
00:16:16,350 --> 00:16:18,619
Great.

219
00:16:19,320 --> 00:16:23,125
So now let's just do that.

220
00:16:23,125 --> 00:16:27,925
So the Q value of
state action pair,

221
00:16:27,925 --> 00:16:29,995
according to Bellman equation,

222
00:16:29,995 --> 00:16:33,025
I only need it to be
the expectation of

223
00:16:33,025 --> 00:16:38,139
R S plus Gamma V S prime.

224
00:16:38,139 --> 00:16:42,529
If you do things
according to S and a.

225
00:16:46,350 --> 00:16:47,769
Correct?

226
00:16:47,769 --> 00:16:52,329
So basically, if I define
q-values to be this,

227
00:16:52,329 --> 00:16:55,134
then I can plug it in here

228
00:16:55,134 --> 00:16:59,029
and recover the
Bellman equation.

229
00:17:00,400 --> 00:17:02,850
Right?

230
00:17:03,190 --> 00:17:07,715
And again, keep in mind
that this makes sense.

231
00:17:07,715 --> 00:17:11,074
According to expectimax, what
they were doing expectimax

232
00:17:11,074 --> 00:17:14,780
is you're trying to put
a number here, right?

233
00:17:14,780 --> 00:17:16,340
And then number is really

234
00:17:16,340 --> 00:17:19,385
the expectation
of these numbers.

235
00:17:19,385 --> 00:17:25,280
The expectation of what
happens at the next state.

236
00:17:25,280 --> 00:17:29,359
And the only difference now
is I consider rewards at

237
00:17:29,359 --> 00:17:33,994
every step I can take and
there's a discount factor.

238
00:17:33,994 --> 00:17:35,899
So the only difference between

239
00:17:35,899 --> 00:17:37,520
the two antibiotics
and here is you

240
00:17:37,520 --> 00:17:38,900
can collect rewards that

241
00:17:38,900 --> 00:17:41,254
every state and you
have a discount factor.

242
00:17:41,254 --> 00:17:43,490
Other than that, you're
just keeping track of what

243
00:17:43,490 --> 00:17:46,624
you were computing for 2048.

244
00:17:46,624 --> 00:17:48,710
And now we give it a name,

245
00:17:48,710 --> 00:17:50,789
call it Q values.

246
00:17:50,830 --> 00:17:53,159
Okay?

247
00:17:53,830 --> 00:18:00,139
And then if I want to take
even one step further to just

248
00:18:00,139 --> 00:18:03,229
not see the state value

249
00:18:03,229 --> 00:18:07,549
at all because it is the
maximization of q-values anyway.

250
00:18:07,549 --> 00:18:08,840
Right?

251
00:18:08,840 --> 00:18:10,355
So I can just rewrite

252
00:18:10,355 --> 00:18:14,030
my whole Bellman equation
with only Q values,

253
00:18:14,030 --> 00:18:18,215
which will be the expectation of

254
00:18:18,215 --> 00:18:23,280
R S plus gamma times what?

255
00:18:23,880 --> 00:18:29,604
The maximum of a
at the next state.

256
00:18:29,604 --> 00:18:31,209
Correct.

257
00:18:31,209 --> 00:18:35,334
And all your possible
choices add the next state.

258
00:18:35,334 --> 00:18:40,105
So whatever as
prime I have here,

259
00:18:40,105 --> 00:18:42,759
if I want to know
about the number here,

260
00:18:42,759 --> 00:18:47,380
I should think about that
maximum action that you can

261
00:18:47,380 --> 00:18:52,210
take there and then
their futures, right?

262
00:18:52,210 --> 00:18:56,080
So things just happen
recursively down one level.

263
00:18:56,080 --> 00:18:58,634
So all I need to say recursively

264
00:18:58,634 --> 00:19:02,334
is I maximize the action
at the next state,

265
00:19:02,334 --> 00:19:11,360
and then I get what
as primary prime.

266
00:19:11,400 --> 00:19:14,139
Again, this is just
rewriting this,

267
00:19:14,139 --> 00:19:16,809
this, There's nothing
tricky happening here.

268
00:19:16,809 --> 00:19:20,259
And then as an a is

269
00:19:20,259 --> 00:19:26,450
whatever is the
probability dependent now.

270
00:19:28,860 --> 00:19:36,444
Okay? Yeah, good question.

271
00:19:36,444 --> 00:19:39,920
Okay, question for this part.

272
00:19:46,830 --> 00:19:49,615
So first of all, this
is all dependent on,

273
00:19:49,615 --> 00:19:53,179
I kind of have a way
of taking actions.

274
00:19:53,179 --> 00:19:54,724
I'm just writing
actions everywhere.

275
00:19:54,724 --> 00:19:57,365
We haven't given that yet,

276
00:19:57,365 --> 00:19:59,150
but there'll be given mercy.

277
00:19:59,150 --> 00:20:00,510
Go ahead.

278
00:20:00,850 --> 00:20:03,600
That's the question.

279
00:20:11,440 --> 00:20:14,450
We do know all the actions.

280
00:20:14,450 --> 00:20:16,819
The environment
doesn't change for

281
00:20:16,819 --> 00:20:20,374
the fixed MDP that
we are dealing with.

282
00:20:20,374 --> 00:20:23,885
We just don't know the
values of those actions.

283
00:20:23,885 --> 00:20:25,414
We need to play the game,

284
00:20:25,414 --> 00:20:28,265
the game is fixed, the
simulator is given to you.

285
00:20:28,265 --> 00:20:30,394
And you need to somehow

286
00:20:30,394 --> 00:20:33,065
understand which action is
good, which action is back.

287
00:20:33,065 --> 00:20:34,320
Yeah.

288
00:20:38,790 --> 00:20:45,070
Okay, so now what

289
00:20:45,070 --> 00:20:49,375
I'm going to say is I
propose the algorithm to be,

290
00:20:49,375 --> 00:20:51,550
I start from some state.

291
00:20:51,550 --> 00:20:53,814
I somehow take an action.

292
00:20:53,814 --> 00:20:55,449
We'll elaborate on that.

293
00:20:55,449 --> 00:20:59,590
And then I see a
transition to S prime.

294
00:20:59,590 --> 00:21:01,255
Okay?

295
00:21:01,255 --> 00:21:06,699
I have my own understanding
of my Q values.

296
00:21:06,699 --> 00:21:09,339
And I want to use
this one transition

297
00:21:09,339 --> 00:21:11,544
to update my new understanding

298
00:21:11,544 --> 00:21:17,269
about this extra state
action pair I took.

299
00:21:17,770 --> 00:21:25,235
What would you do? So in

300
00:21:25,235 --> 00:21:26,599
temporal difference
learning which you

301
00:21:26,599 --> 00:21:28,309
are okay now what is hopefully,

302
00:21:28,309 --> 00:21:30,650
we say according to a fixed pie,

303
00:21:30,650 --> 00:21:31,820
you look up the action, right?

304
00:21:31,820 --> 00:21:35,759
And then we want to
find out, you know,

305
00:21:39,820 --> 00:21:44,434
from the ODE as prime,

306
00:21:44,434 --> 00:21:46,219
I want to get we

307
00:21:46,219 --> 00:21:50,540
knew right frontal view

308
00:21:50,540 --> 00:21:53,069
owed on his crown
and build on it.

309
00:21:55,240 --> 00:21:58,220
Okay, so that's what we did

310
00:21:58,220 --> 00:22:02,045
for temporal difference
learning was a fixed pie.

311
00:22:02,045 --> 00:22:08,219
And now what I want
to do is Q as crime.

312
00:22:08,470 --> 00:22:16,579
Somehow a prime and
Q S and a 0, 0.

313
00:22:16,579 --> 00:22:19,699
I want to use that to know.

314
00:22:19,699 --> 00:22:22,534
Now what should I understand

315
00:22:22,534 --> 00:22:26,850
about my particular
state and action choice?

316
00:22:27,250 --> 00:22:30,680
Again, the problem setup
is exactly the same.

317
00:22:30,680 --> 00:22:36,244
It's just that now I have this
additional action, right?

318
00:22:36,244 --> 00:22:38,450
And I want to go from

319
00:22:38,450 --> 00:22:43,270
my previous or
current old estimate

320
00:22:43,270 --> 00:22:47,149
of the values to get new values.

321
00:22:47,760 --> 00:22:52,639
What would you
suggest that we do?

322
00:23:24,510 --> 00:23:26,484
Now?

323
00:23:26,484 --> 00:23:28,370
Yeah.

324
00:23:39,150 --> 00:23:41,419
Yeah.

325
00:23:42,990 --> 00:23:47,124
That's what we want to
do. So first of all,

326
00:23:47,124 --> 00:23:49,464
from the Bellman equation
for the state values,

327
00:23:49,464 --> 00:23:54,084
you're all k was the Bellman
equation for the Q values.

328
00:23:54,084 --> 00:23:55,419
Right?

329
00:23:55,419 --> 00:23:57,490
Now let's just pretend you've

330
00:23:57,490 --> 00:23:59,740
never heard about state values.

331
00:23:59,740 --> 00:24:03,985
I'll you know, all you
have are these q-values.

332
00:24:03,985 --> 00:24:06,970
It's always over
state action pairs.

333
00:24:06,970 --> 00:24:11,274
And we came up with that
temporal difference formula.

334
00:24:11,274 --> 00:24:13,584
When everything is overstates,

335
00:24:13,584 --> 00:24:21,895
was exactly the structure
of S is the expectation

336
00:24:21,895 --> 00:24:27,310
of R S plus Gamma v as

337
00:24:27,310 --> 00:24:33,669
prime for a fixed pie, right?

338
00:24:33,669 --> 00:24:34,810
So we had that.

339
00:24:34,810 --> 00:24:36,910
And then from this we say, okay,

340
00:24:36,910 --> 00:24:39,459
this is estimating
an expectation.

341
00:24:39,459 --> 00:24:41,259
So let's look at one
day the point of

342
00:24:41,259 --> 00:24:43,239
this and compare it with

343
00:24:43,239 --> 00:24:45,669
your current understanding
and use that to

344
00:24:45,669 --> 00:24:47,439
multiply it with
the learning rate

345
00:24:47,439 --> 00:24:49,229
to update your
understanding, right?

346
00:24:49,229 --> 00:24:51,049
So you're okay with that.

347
00:24:51,049 --> 00:24:54,965
Now, all that we're
changing this to is

348
00:24:54,965 --> 00:25:00,785
q essay is the expectation.

349
00:25:00,785 --> 00:25:08,135
Again, RS plus max of a prime,

350
00:25:08,135 --> 00:25:12,119
Q prime, a prime.

351
00:25:12,610 --> 00:25:17,509
Okay? And in the
state value case,

352
00:25:17,509 --> 00:25:18,815
I wanted to update this.

353
00:25:18,815 --> 00:25:20,989
And now I want to update this.

354
00:25:20,989 --> 00:25:26,270
The algorithm should be
very similar, Correct?

355
00:25:26,270 --> 00:25:28,379
What would you do?

356
00:25:43,870 --> 00:25:46,110
Yeah.

357
00:25:49,630 --> 00:25:58,549
Yes. So the update rule I would

358
00:25:58,549 --> 00:26:07,909
propose is Q as a
new should get.

359
00:26:07,909 --> 00:26:09,860
First of all, I look at

360
00:26:09,860 --> 00:26:14,944
this one-step experience
going from S to S prime.

361
00:26:14,944 --> 00:26:18,335
After a look at this,

362
00:26:18,335 --> 00:26:20,060
once that data point,

363
00:26:20,060 --> 00:26:23,690
which is the thing I can collect
inside this expectation,

364
00:26:23,690 --> 00:26:25,385
which is what we
are trying to take

365
00:26:25,385 --> 00:26:27,710
running average earth, right?

366
00:26:27,710 --> 00:26:32,989
I look at R of S. Plus.

367
00:26:32,989 --> 00:26:38,180
The only tricky thing is this
max that you are afraid of.

368
00:26:38,180 --> 00:26:40,069
But really, it doesn't

369
00:26:40,069 --> 00:26:42,125
matter because everything
is you're saying,

370
00:26:42,125 --> 00:26:43,789
oh, the Q values.

371
00:26:43,789 --> 00:26:47,974
So I have them write
disliking state value case.

372
00:26:47,974 --> 00:26:49,460
I assume I started with just

373
00:26:49,460 --> 00:26:51,215
random guessing all the states.

374
00:26:51,215 --> 00:26:54,784
And for Q is I start from
random guys and all the cues,

375
00:26:54,784 --> 00:26:56,914
whether update or not,
they're just there.

376
00:26:56,914 --> 00:26:59,239
You have the matrix over essay.

377
00:26:59,239 --> 00:27:00,620
Now, it used to be

378
00:27:00,620 --> 00:27:03,395
just a bunch of numbers
for your V of S.

379
00:27:03,395 --> 00:27:04,984
Now there's a bigger table,

380
00:27:04,984 --> 00:27:06,529
but you still have them starting

381
00:27:06,529 --> 00:27:08,825
from all zeros aren't
brand whatever you want.

382
00:27:08,825 --> 00:27:11,689
Whenever you ask questions about

383
00:27:11,689 --> 00:27:14,525
what is the max a prime
for the next state,

384
00:27:14,525 --> 00:27:17,060
you just look at whatever
current guess you have.

385
00:27:17,060 --> 00:27:18,529
This is something
that you can always

386
00:27:18,529 --> 00:27:23,314
do according to your
current old value.

387
00:27:23,314 --> 00:27:28,264
So all you need to do is first,

388
00:27:28,264 --> 00:27:34,010
of course, lookup where your
next state Q value entry is.

389
00:27:34,010 --> 00:27:41,359
And then take the
max of that. Again.

390
00:27:41,359 --> 00:27:43,865
Whenever you view.

391
00:27:43,865 --> 00:27:47,090
The max operator is annoying.

392
00:27:47,090 --> 00:27:49,790
Just keeping in mind, the
only reason we're writing

393
00:27:49,790 --> 00:27:54,724
this is to pretend that
we only have two values.

394
00:27:54,724 --> 00:28:01,370
But if you actually keep as
the value for your S prime,

395
00:28:01,370 --> 00:28:03,889
that is also something
you can just look up.

396
00:28:03,889 --> 00:28:05,810
If you just have another
lookup table that

397
00:28:05,810 --> 00:28:08,435
always maximize whatever
actions you have,

398
00:28:08,435 --> 00:28:11,585
an essay that is your
best to stay value.

399
00:28:11,585 --> 00:28:14,075
You can just plug in
that as well, okay?

400
00:28:14,075 --> 00:28:15,169
But you really don't need to do

401
00:28:15,169 --> 00:28:16,609
that and you just need to keep

402
00:28:16,609 --> 00:28:18,290
one lookup table and every

403
00:28:18,290 --> 00:28:20,405
time you need to query
for the next state,

404
00:28:20,405 --> 00:28:21,964
take the best possible thing.

405
00:28:21,964 --> 00:28:25,490
You don't want to be
pessimistic there, right?

406
00:28:25,490 --> 00:28:29,550
Because we're looking for
the best possible, right?

407
00:28:30,250 --> 00:28:35,510
And then look at the
difference between that

408
00:28:35,510 --> 00:28:36,950
and my

409
00:28:36,950 --> 00:28:47,489
current q-value.

410
00:28:48,010 --> 00:28:50,494
Look at their difference.

411
00:28:50,494 --> 00:28:54,335
And this is in principle
how you should modify

412
00:28:54,335 --> 00:28:58,054
your previous understanding of

413
00:28:58,054 --> 00:29:01,145
this particular
state action pair.

414
00:29:01,145 --> 00:29:07,385
Pick this difference,
multiply worse than Alpha,

415
00:29:07,385 --> 00:29:15,530
which now depends on the visits
to the state action pair.

416
00:29:15,530 --> 00:29:16,219
Right?

417
00:29:16,219 --> 00:29:18,229
Now, the state action pairs
are the things that you

418
00:29:18,229 --> 00:29:21,379
care about knowing
the statistics off.

419
00:29:21,379 --> 00:29:22,564
Okay?

420
00:29:22,564 --> 00:29:24,484
So there may be
some state if you

421
00:29:24,484 --> 00:29:27,140
start with a reasonable
enough policy,

422
00:29:27,140 --> 00:29:31,085
if you're a 1010, probably in
most cases, you just well,

423
00:29:31,085 --> 00:29:33,440
you always stand right then

424
00:29:33,440 --> 00:29:36,139
probably you would
with it the pair

425
00:29:36,139 --> 00:29:42,514
of plenty and stand much
more often than 20 and hit.

426
00:29:42,514 --> 00:29:45,154
Right? So for the same state,

427
00:29:45,154 --> 00:29:48,200
you may have different visitation
for different actions.

428
00:29:48,200 --> 00:29:48,829
That's fine.

429
00:29:48,829 --> 00:29:50,629
Whatever it is, just use

430
00:29:50,629 --> 00:29:55,325
that number and turn that
into a learning rate.

431
00:29:55,325 --> 00:29:56,599
Okay.

432
00:29:56,599 --> 00:30:03,560
And that is the increment
you want to do for

433
00:30:03,560 --> 00:30:05,494
your current understanding of

434
00:30:05,494 --> 00:30:12,829
the cube? That's how you update.

435
00:30:12,829 --> 00:30:14,819
Yeah.

436
00:30:17,890 --> 00:30:19,264
Yes.

437
00:30:19,264 --> 00:30:21,769
How many times you actually try

438
00:30:21,769 --> 00:30:25,399
out this particular action
when you see that state.

439
00:30:25,399 --> 00:30:27,455
Again.

440
00:30:27,455 --> 00:30:31,100
The straightforward way
is really just the number

441
00:30:31,100 --> 00:30:34,685
of visits to that state.

442
00:30:34,685 --> 00:30:36,499
One divided by that.

443
00:30:36,499 --> 00:30:38,435
Yeah. Just like one over K.

444
00:30:38,435 --> 00:30:39,840
Go ahead.

445
00:30:46,720 --> 00:30:51,079
Yes. So that's a
very good point.

446
00:30:51,079 --> 00:30:55,325
That's a we're going to
spend the whole next week.

447
00:30:55,325 --> 00:31:02,099
So first, every symbol we
spent at least one week.

448
00:31:02,470 --> 00:31:06,440
The more you try out
a particular action,

449
00:31:06,440 --> 00:31:10,230
the better you know about it.

450
00:31:10,450 --> 00:31:14,734
The more you try out a
particular state action pair,

451
00:31:14,734 --> 00:31:16,804
the less this alpha
is going to get,

452
00:31:16,804 --> 00:31:19,760
which means the last new
experience appearance

453
00:31:19,760 --> 00:31:22,160
is going to matter, right?

454
00:31:22,160 --> 00:31:24,214
And at the same time,

455
00:31:24,214 --> 00:31:27,200
if you have a very bad choice,

456
00:31:27,200 --> 00:31:31,385
you probably don't want
to visit it very often,

457
00:31:31,385 --> 00:31:33,604
just so that you
know more about it.

458
00:31:33,604 --> 00:31:36,289
If you know some
option is bad enough,

459
00:31:36,289 --> 00:31:41,014
you probably want to limit
your visits to that,

460
00:31:41,014 --> 00:31:43,640
which at the same
time will also limit

461
00:31:43,640 --> 00:31:46,639
your understanding about
that potentially bad choice.

462
00:31:46,639 --> 00:31:48,980
So if you know nothing,

463
00:31:48,980 --> 00:31:51,215
there is a trade-off

464
00:31:51,215 --> 00:31:55,009
between going was the
best thing that you

465
00:31:55,009 --> 00:31:58,189
currently see versus going

466
00:31:58,189 --> 00:32:00,349
to something that's not
the best that you see,

467
00:32:00,349 --> 00:32:03,665
but you expect that
to be potential,

468
00:32:03,665 --> 00:32:06,905
meaning that maybe
your few samples

469
00:32:06,905 --> 00:32:09,845
in the beginning are not.

470
00:32:09,845 --> 00:32:15,050
Representative enough for
its true expectation.

471
00:32:15,050 --> 00:32:18,154
So this is the
fundamental trade-off

472
00:32:18,154 --> 00:32:21,604
called exploration
versus exploitation.

473
00:32:21,604 --> 00:32:24,739
Exploitation means
you go for the action

474
00:32:24,739 --> 00:32:26,090
that for the time being you

475
00:32:26,090 --> 00:32:28,145
believe to be the to
be the best thing.

476
00:32:28,145 --> 00:32:30,214
But if that's the
only thing you do,

477
00:32:30,214 --> 00:32:32,900
you will miss out on
other things that you

478
00:32:32,900 --> 00:32:36,320
probably just didn't know
enough about, right?

479
00:32:36,320 --> 00:32:39,485
So exploration will be
the other side, which is,

480
00:32:39,485 --> 00:32:41,375
Let's just try to maximize

481
00:32:41,375 --> 00:32:44,105
our understanding
about all the options.

482
00:32:44,105 --> 00:32:47,615
Rather, don t think about
whether they're good or bad.

483
00:32:47,615 --> 00:32:48,649
Okay?

484
00:32:48,649 --> 00:32:50,914
So now there's two objectives.

485
00:32:50,914 --> 00:32:52,520
One is to maximize

486
00:32:52,520 --> 00:32:55,234
your understanding or information
about all the choices.

487
00:32:55,234 --> 00:32:58,160
The other is you still
want to maximize

488
00:32:58,160 --> 00:33:01,055
your overall rewards, right?

489
00:33:01,055 --> 00:33:02,254
You go into a casino.

490
00:33:02,254 --> 00:33:04,535
Yes, you are trying to
learn to play blackjack,

491
00:33:04,535 --> 00:33:09,395
but also you have a fixed
budget. Then what do you do?

492
00:33:09,395 --> 00:33:11,164
That is the trade-off.

493
00:33:11,164 --> 00:33:18,245
That will be the topic of
art, mainly next chapter.

494
00:33:18,245 --> 00:33:23,374
But here you see a
flavor of that, right?

495
00:33:23,374 --> 00:33:27,710
And that actually
gives us the policy.

496
00:33:27,710 --> 00:33:31,460
So when you are choosing

497
00:33:31,460 --> 00:33:34,010
actions based on your Q values

498
00:33:34,010 --> 00:33:36,930
that you're updating
dynamically.

499
00:33:37,210 --> 00:33:40,340
You want to go with
the actions that

500
00:33:40,340 --> 00:33:42,589
are as good as
possible in the sense

501
00:33:42,589 --> 00:33:48,320
that you want to get as much
total rewards as possible.

502
00:33:48,320 --> 00:33:50,134
And at the same time,

503
00:33:50,134 --> 00:33:54,589
you want to always leave
out some probabilities are

504
00:33:54,589 --> 00:33:58,039
some opportunities
for actions that

505
00:33:58,039 --> 00:34:02,969
currently seem bad
in terms of q-value.

506
00:34:03,630 --> 00:34:05,664
Alright?

507
00:34:05,664 --> 00:34:09,470
What, what would
you propose to do?

508
00:34:09,570 --> 00:34:14,020
If that's the thing
you want to balance?

509
00:34:14,020 --> 00:34:16,659
So that's just okay.

510
00:34:16,659 --> 00:34:19,285
So this, the queue add
date is fine now, alright?

511
00:34:19,285 --> 00:34:22,585
So this is just whenever you
somehow choose an action,

512
00:34:22,585 --> 00:34:24,700
you just look at
whatever Q values are

513
00:34:24,700 --> 00:34:27,009
for the next date and then
use that to update things.

514
00:34:27,009 --> 00:34:28,539
And since this is
Bellman equation,

515
00:34:28,539 --> 00:34:29,619
we know if we do things

516
00:34:29,619 --> 00:34:31,270
lean this way, it's
going to converge.

517
00:34:31,270 --> 00:34:34,779
Now, the only question left
is how we choose the action?

518
00:34:34,779 --> 00:34:36,079
Go ahead.

519
00:34:37,080 --> 00:34:39,319
Yeah.

520
00:34:48,940 --> 00:34:53,224
You mean which are the same?

521
00:34:53,224 --> 00:34:56,429
These two things
are these and this.

522
00:35:16,240 --> 00:35:19,445
So when you do this update,

523
00:35:19,445 --> 00:35:26,839
you're changing this one
entry in your q table, right?

524
00:35:26,839 --> 00:35:28,940
Everything is a lookup.

525
00:35:28,940 --> 00:35:31,774
And the only thing
you are changing is

526
00:35:31,774 --> 00:35:35,899
this state action pair that
you are trying out right now.

527
00:35:35,899 --> 00:35:36,934
Okay?

528
00:35:36,934 --> 00:35:42,260
So you have these whole q
table for all your state law,

529
00:35:42,260 --> 00:35:45,245
your actions there,
this one cell

530
00:35:45,245 --> 00:35:48,980
that you're changing
was this, right?

531
00:35:48,980 --> 00:35:51,634
And then the change to that

532
00:35:51,634 --> 00:35:54,380
is determined by this formula,

533
00:35:54,380 --> 00:35:57,455
which looks up this value

534
00:35:57,455 --> 00:36:00,259
and looks up your
current value for this.

535
00:36:00,259 --> 00:36:01,970
And these are two very
different numbers

536
00:36:01,970 --> 00:36:04,865
potentially. Alright?

537
00:36:04,865 --> 00:36:09,179
And then just do whatever
this formula is saying.

538
00:36:09,340 --> 00:36:12,710
So there's one step
update only changes

539
00:36:12,710 --> 00:36:16,325
one entry in your
queue lookup table.

540
00:36:16,325 --> 00:36:17,850
Good.

541
00:36:19,630 --> 00:36:21,899
Yeah,

542
00:36:29,770 --> 00:36:31,459
sure.

543
00:36:31,459 --> 00:36:34,229
I have 1010.

544
00:36:34,300 --> 00:36:38,284
And I know nothing
about the game.

545
00:36:38,284 --> 00:36:41,660
The first time I tried it,
I asked for a new card.

546
00:36:41,660 --> 00:36:44,240
Somehow I get a one that can

547
00:36:44,240 --> 00:36:49,550
happen in my world and this
particular round, right?

548
00:36:49,550 --> 00:36:53,465
So from that, I
understand the Q value of

549
00:36:53,465 --> 00:37:00,990
1010 given hit is really high.

550
00:37:01,990 --> 00:37:05,989
I have one transition
from this to tan,

551
00:37:05,989 --> 00:37:09,095
tan, tan, tan, tan one.

552
00:37:09,095 --> 00:37:16,535
And when? That's all I ever
have seen in blackjack.

553
00:37:16,535 --> 00:37:17,074
Okay.

554
00:37:17,074 --> 00:37:19,009
Based on that, no one

555
00:37:19,009 --> 00:37:21,824
can say My understanding
of the game is wrong.

556
00:37:21,824 --> 00:37:26,110
It's totally true based on my
update because I go there,

557
00:37:26,110 --> 00:37:27,444
this becomes a terminal state.

558
00:37:27,444 --> 00:37:31,434
I get my money and
I update this based

559
00:37:31,434 --> 00:37:35,814
on the next maximum is the
amount I'm getting, right?

560
00:37:35,814 --> 00:37:37,149
So the Q value of

561
00:37:37,149 --> 00:37:38,755
this particular
state action pair

562
00:37:38,755 --> 00:37:42,069
is positive, at least, right?

563
00:37:42,069 --> 00:37:49,570
And this is definitely
higher than q can stand,

564
00:37:49,570 --> 00:37:53,690
which I came in with a
random guess of zero.

565
00:37:56,220 --> 00:37:58,389
So then what do you do?

566
00:37:58,389 --> 00:38:00,594
If you encounter
this date again?

567
00:38:00,594 --> 00:38:04,575
You for them too?

568
00:38:04,575 --> 00:38:09,394
Infinity, always
ask for a new card.

569
00:38:09,394 --> 00:38:10,774
You don't.

570
00:38:10,774 --> 00:38:11,645
Right?

571
00:38:11,645 --> 00:38:15,184
So the difficulty of exploration

572
00:38:15,184 --> 00:38:18,860
versus exploitation
is really crucial,

573
00:38:18,860 --> 00:38:21,755
dependent on the world
being stochastic.

574
00:38:21,755 --> 00:38:24,244
Your sample may never,

575
00:38:24,244 --> 00:38:28,655
your smart number of
samples may never give you

576
00:38:28,655 --> 00:38:35,134
a precise enough understanding
about the transition.

577
00:38:35,134 --> 00:38:40,205
But what you can hope for
is if you have played

578
00:38:40,205 --> 00:38:42,695
this action at this state

579
00:38:42,695 --> 00:38:46,445
for a reasonable
number of times,

580
00:38:46,445 --> 00:38:49,025
hit is still better than stand.

581
00:38:49,025 --> 00:38:51,725
It both have been played
100 times, for instance,

582
00:38:51,725 --> 00:38:53,795
and hit is better,

583
00:38:53,795 --> 00:38:55,895
which will not happen, right?

584
00:38:55,895 --> 00:38:57,545
But if that's the case,

585
00:38:57,545 --> 00:38:58,939
then you basically have

586
00:38:58,939 --> 00:39:01,819
statistical guarantees
saying I have simple enough

587
00:39:01,819 --> 00:39:03,290
so that I have reached

588
00:39:03,290 --> 00:39:06,440
a stable understanding of
that particular option.

589
00:39:06,440 --> 00:39:08,479
And based on that, I
choose the better one.

590
00:39:08,479 --> 00:39:12,664
The more times you visit that
action state action pair,

591
00:39:12,664 --> 00:39:15,215
the more reliable
your experience.

592
00:39:15,215 --> 00:39:15,680
Yes.

593
00:39:15,680 --> 00:39:18,920
And that is why the more
wisdom so you have,

594
00:39:18,920 --> 00:39:22,890
the less alpha
we're going to use.

595
00:39:28,210 --> 00:39:31,639
Yes, there'll be one choice.

596
00:39:31,639 --> 00:39:35,550
Right? Yeah.

597
00:39:38,740 --> 00:39:44,489
Stabilizes higher than your
initial guess, probably.

598
00:39:44,980 --> 00:39:48,815
Wouldn't you, in future
classes save it,

599
00:39:48,815 --> 00:39:51,179
then you would get
punished for it.

600
00:39:54,400 --> 00:39:56,960
So the question is,

601
00:39:56,960 --> 00:40:02,839
if I just insist not trying
the other option, right?

602
00:40:02,839 --> 00:40:06,530
Do I still okay.

603
00:40:06,530 --> 00:40:08,645
To just keep
exploiting one action

604
00:40:08,645 --> 00:40:12,300
and formulae in a
policy like that?

605
00:40:18,310 --> 00:40:20,579
Yeah.

606
00:40:22,330 --> 00:40:23,855
Yes.

607
00:40:23,855 --> 00:40:27,170
Let's say you do
this and then you

608
00:40:27,170 --> 00:40:32,305
say after ten times I get a
minus as five or minus nine.

609
00:40:32,305 --> 00:40:34,344
Okay.

610
00:40:34,344 --> 00:40:37,210
And the question
is, is that enough?

611
00:40:37,210 --> 00:40:39,969
The answer is depends
on what you came in.

612
00:40:39,969 --> 00:40:41,889
And was that the
estimate for that?

613
00:40:41,889 --> 00:40:46,059
What if I came in and say
everything started from -20?

614
00:40:46,059 --> 00:40:50,980
I'm allowed to do that
without any sample at all.

615
00:40:50,980 --> 00:40:53,454
You really have no idea at all.

616
00:40:53,454 --> 00:40:55,460
Good.

617
00:40:56,490 --> 00:41:00,560
Yeah, partially. We
need to do that.

618
00:41:04,320 --> 00:41:05,890
Okay.

619
00:41:05,890 --> 00:41:07,839
We're going to keep
talking about that,

620
00:41:07,839 --> 00:41:11,885
but other questions
about for right now.

621
00:41:11,885 --> 00:41:16,679
But all these discussion
about the action to

622
00:41:16,679 --> 00:41:22,844
take already gave us the default
way of doing Q-learning,

623
00:41:22,844 --> 00:41:29,890
which is called
epsilon greedy policy.

624
00:41:31,160 --> 00:41:33,495
And you already have the,

625
00:41:33,495 --> 00:41:35,054
have the name for it,

626
00:41:35,054 --> 00:41:37,905
which is every time suppose,

627
00:41:37,905 --> 00:41:40,155
whenever I need to
take an action,

628
00:41:40,155 --> 00:41:42,359
I just look at my
current q-value on

629
00:41:42,359 --> 00:41:45,240
each action and go
with the maximum one.

630
00:41:45,240 --> 00:41:46,785
Go is the best one,

631
00:41:46,785 --> 00:41:49,424
then that is what
we can call greedy.

632
00:41:49,424 --> 00:41:52,799
So at all times you
have a queue SMA

633
00:41:52,799 --> 00:41:54,299
to starting from anything

634
00:41:54,299 --> 00:41:56,399
and then you gradually
update them.

635
00:41:56,399 --> 00:41:59,625
At all times you have
that comparison to make.

636
00:41:59,625 --> 00:42:02,189
And you can always
take the maximum,

637
00:42:02,189 --> 00:42:05,459
the best action that
maximizes the Q value.

638
00:42:05,459 --> 00:42:07,289
So that's called greedy.

639
00:42:07,289 --> 00:42:09,929
And then at same time, we said,

640
00:42:09,929 --> 00:42:11,834
don't always do that

641
00:42:11,834 --> 00:42:14,250
because you don't
really know what is

642
00:42:14,250 --> 00:42:18,599
the statistical ground truth
of all your other actions.

643
00:42:18,599 --> 00:42:22,050
You may just be unlucky and
happened to see a bunch

644
00:42:22,050 --> 00:42:23,579
of samples that are not

645
00:42:23,579 --> 00:42:25,739
representative of
their true potential.

646
00:42:25,739 --> 00:42:26,370
Alright?

647
00:42:26,370 --> 00:42:27,989
So always leave Alice and

648
00:42:27,989 --> 00:42:30,584
probabilities for other actions.

649
00:42:30,584 --> 00:42:38,879
So I have some greedy is
just saying with Amazon

650
00:42:38,879 --> 00:42:48,284
being 020 to 0.5, Let's see.

651
00:42:48,284 --> 00:42:49,814
Okay.

652
00:42:49,814 --> 00:42:51,824
It can be, well,

653
00:42:51,824 --> 00:42:53,849
I don't recommend
setting it to be

654
00:42:53,849 --> 00:42:56,130
exactly zero or exactly 0.5.

655
00:42:56,130 --> 00:42:57,690
So let's just say in-between.

656
00:42:57,690 --> 00:42:59,610
So with one minus epsilon,

657
00:42:59,610 --> 00:43:05,625
which is the slightly higher
probability play action

658
00:43:05,625 --> 00:43:12,040
that maximizes Q essay.

659
00:43:12,200 --> 00:43:18,460
And with epsilon probability,

660
00:43:18,470 --> 00:43:21,609
try out other things.

661
00:43:29,840 --> 00:43:31,350
That's it.

662
00:43:31,350 --> 00:43:34,740
That's the answer of,

663
00:43:34,740 --> 00:43:36,944
you know, in Q-learning,

664
00:43:36,944 --> 00:43:40,029
what should I do here?

665
00:43:49,280 --> 00:43:50,850
Okay?

666
00:43:50,850 --> 00:43:52,290
So this is

667
00:43:52,290 --> 00:43:57,464
sort of an intuitive and
straightforward or naive way

668
00:43:57,464 --> 00:44:03,210
of balancing the exploration
versus exploitation, right?

669
00:44:03,210 --> 00:44:05,099
I just say, let me just give

670
00:44:05,099 --> 00:44:07,874
a fixed higher probability

671
00:44:07,874 --> 00:44:11,520
to the currently better action.

672
00:44:11,520 --> 00:44:14,535
And hopefully by
playing that action,

673
00:44:14,535 --> 00:44:18,839
I will get more rewards, right?

674
00:44:18,839 --> 00:44:20,835
And then at the same time,

675
00:44:20,835 --> 00:44:25,409
always leave out
the opportunities

676
00:44:25,409 --> 00:44:28,724
for other actions
because you never know.

677
00:44:28,724 --> 00:44:31,889
So assign some small probability

678
00:44:31,889 --> 00:44:34,899
to always explore
something else.

679
00:44:34,940 --> 00:44:37,860
Now, this is good

680
00:44:37,860 --> 00:44:40,574
enough because in
reinforcement learning,

681
00:44:40,574 --> 00:44:45,119
you don't really care
about how fast you learn.

682
00:44:45,119 --> 00:44:47,729
All we talked about is
you do things, you know,

683
00:44:47,729 --> 00:44:49,229
temporal difference
and eventually

684
00:44:49,229 --> 00:44:51,090
they converge the right thing.

685
00:44:51,090 --> 00:44:53,550
And now we know this
will converge to

686
00:44:53,550 --> 00:44:55,770
the right thing because even if

687
00:44:55,770 --> 00:45:00,285
we leave out a small probability
for the bad actions,

688
00:45:00,285 --> 00:45:02,115
if time goes to infinity,

689
00:45:02,115 --> 00:45:04,034
you're always going
to visit them

690
00:45:04,034 --> 00:45:06,585
for enough number of times.

691
00:45:06,585 --> 00:45:07,979
So the visitation to

692
00:45:07,979 --> 00:45:12,989
all state action pairs
is always infinite.

693
00:45:12,989 --> 00:45:17,205
Because I have non-zero
probability for each choice.

694
00:45:17,205 --> 00:45:21,254
Then I would know this update

695
00:45:21,254 --> 00:45:23,189
for every state action
pair is going to

696
00:45:23,189 --> 00:45:25,184
happen for a long enough time.

697
00:45:25,184 --> 00:45:26,820
And we know because
of contraction,

698
00:45:26,820 --> 00:45:29,894
as long as you allow
it to go enough times,

699
00:45:29,894 --> 00:45:31,560
it will converge to
the right thing.

700
00:45:31,560 --> 00:45:39,660
Good for our PA
and logistics it.

701
00:45:39,660 --> 00:45:42,585
In reality, shrinking it.

702
00:45:42,585 --> 00:45:44,205
Or actually in reality,

703
00:45:44,205 --> 00:45:46,364
what you would do is you do

704
00:45:46,364 --> 00:45:47,610
whatever absolute choice you

705
00:45:47,610 --> 00:45:50,055
get in the Q-learning process.

706
00:45:50,055 --> 00:45:52,590
And then when you actually
need to run the policy,

707
00:45:52,590 --> 00:45:55,229
you basically make
it deterministic.

708
00:45:55,229 --> 00:45:57,265
You always go for
the greedy one.

709
00:45:57,265 --> 00:46:01,199
So basically the probability
on other things allows

710
00:46:01,199 --> 00:46:04,860
you to gather more information
or do more learning.

711
00:46:04,860 --> 00:46:07,019
And then when you are asked to

712
00:46:07,019 --> 00:46:09,030
stop and just play
the blackjack,

713
00:46:09,030 --> 00:46:11,295
blackjack was the
best policy you have.

714
00:46:11,295 --> 00:46:14,324
You would just do
epsilon being zero.

715
00:46:14,324 --> 00:46:16,650
And we know by doing this,

716
00:46:16,650 --> 00:46:19,499
you will be able to
separate the true q values,

717
00:46:19,499 --> 00:46:22,349
the good actions versus
the bad actions.

718
00:46:22,349 --> 00:46:24,899
And if you always
go with the ground,

719
00:46:24,899 --> 00:46:26,700
choose best to Q value,

720
00:46:26,700 --> 00:46:29,639
you will reach the
optimal policy.

721
00:46:29,639 --> 00:46:32,504
So by doing Q-learning
and this way again,

722
00:46:32,504 --> 00:46:34,049
choosing actions space down

723
00:46:34,049 --> 00:46:35,219
your temporary guests of

724
00:46:35,219 --> 00:46:37,290
the q-values and
epsilon greedy way,

725
00:46:37,290 --> 00:46:39,390
and then update your Q values,

726
00:46:39,390 --> 00:46:41,400
which is updating
your estate values

727
00:46:41,400 --> 00:46:43,334
based on whatever
current policy you have.

728
00:46:43,334 --> 00:46:45,720
Eventually this whole
procedure is going to

729
00:46:45,720 --> 00:46:48,914
give you the best
possible policy.

730
00:46:48,914 --> 00:46:52,079
That's the answer to

731
00:46:52,079 --> 00:46:54,059
the whole reinforced
learning problem in

732
00:46:54,059 --> 00:46:57,269
this setting of finite
and discrete state

733
00:46:57,269 --> 00:46:59,609
and finite discrete
action space.

734
00:46:59,609 --> 00:47:01,170
Okay?

735
00:47:01,170 --> 00:47:04,304
So Q-learning is in some sense

736
00:47:04,304 --> 00:47:07,050
a perfect algorithm
if you don't care so

737
00:47:07,050 --> 00:47:10,155
much about the
running time, right?

738
00:47:10,155 --> 00:47:12,119
Because it doesn't
require you to

739
00:47:12,119 --> 00:47:15,540
know anything about the MDP.

740
00:47:15,540 --> 00:47:18,539
Well, anything about the
transition function, right?

741
00:47:18,539 --> 00:47:21,330
So we don't assume any knowledge
about the probability.

742
00:47:21,330 --> 00:47:24,840
We just say that we know
is there and is fixed.

743
00:47:24,840 --> 00:47:27,015
All we need to do is to sample

744
00:47:27,015 --> 00:47:30,285
transitions from
the MDP based on

745
00:47:30,285 --> 00:47:33,779
this explicit choice
of actions is

746
00:47:33,779 --> 00:47:37,815
a very practical algorithm.

747
00:47:37,815 --> 00:47:40,440
You can implement it.
And then we know based

748
00:47:40,440 --> 00:47:43,125
on Bellman equation
contraction of the law.

749
00:47:43,125 --> 00:47:44,580
In the end, it's always going to

750
00:47:44,580 --> 00:47:47,650
converge to the right answer.

751
00:47:47,650 --> 00:47:49,850
The right answer being finding

752
00:47:49,850 --> 00:47:52,129
the optimal action
at every state,

753
00:47:52,129 --> 00:47:56,070
which means finding
the optimal policy.

754
00:47:56,380 --> 00:48:00,890
Okay? So that fully
answers how to

755
00:48:00,890 --> 00:48:05,850
play blackjack
after three weeks.

756
00:48:06,610 --> 00:48:07,925
Okay?

757
00:48:07,925 --> 00:48:10,549
So this is the third
algorithm you are supposed to

758
00:48:10,549 --> 00:48:14,399
implement in that PA.

759
00:48:19,000 --> 00:48:21,454
Questions about that.

760
00:48:21,454 --> 00:48:23,674
So first of all,
let me ask again.

761
00:48:23,674 --> 00:48:30,150
What is the Q value
of a terminal state?

762
00:48:31,870 --> 00:48:35,764
Which doesn't quite
make sense, right?

763
00:48:35,764 --> 00:48:39,870
Yeah, go ahead. Yeah. Right.

764
00:48:40,120 --> 00:48:42,229
Yeah, exactly.

765
00:48:42,229 --> 00:48:42,574
Right.

766
00:48:42,574 --> 00:48:47,029
So we talked about
the state value of

767
00:48:47,029 --> 00:48:49,609
the state value and
our fixed policy of

768
00:48:49,609 --> 00:48:50,900
a terminal state will just be

769
00:48:50,900 --> 00:48:52,940
the reward of that state right?

770
00:48:52,940 --> 00:48:55,370
Now. Same thing
for the q values.

771
00:48:55,370 --> 00:48:57,154
First of all, yes,

772
00:48:57,154 --> 00:48:59,959
there's no actions to
take in a terminal state.

773
00:48:59,959 --> 00:49:02,689
So your Q essay is basically

774
00:49:02,689 --> 00:49:06,410
just VFS when S is
just the terminal.

775
00:49:06,410 --> 00:49:08,555
And then every time you visit

776
00:49:08,555 --> 00:49:12,654
a particular state with no
action at the terminal state,

777
00:49:12,654 --> 00:49:14,144
you are going to do this,

778
00:49:14,144 --> 00:49:16,949
update and increment
the estimate that

779
00:49:16,949 --> 00:49:20,760
that q-value a little bit
towards its reward, right?

780
00:49:20,760 --> 00:49:22,229
So after enough time,

781
00:49:22,229 --> 00:49:24,360
you're just going to converge to

782
00:49:24,360 --> 00:49:27,014
the ground truth reward
at the terminal state.

783
00:49:27,014 --> 00:49:30,449
So if you happen to set
your initial guess of

784
00:49:30,449 --> 00:49:32,354
the Q values to be

785
00:49:32,354 --> 00:49:35,399
exactly the reward at
the terminal state.

786
00:49:35,399 --> 00:49:37,900
You are definitely correct.

787
00:49:38,840 --> 00:49:42,614
And we said it
doesn't even matter.

788
00:49:42,614 --> 00:49:45,944
Any initial guess you can use,

789
00:49:45,944 --> 00:49:49,869
but that'll be a
good initialization.

790
00:49:51,410 --> 00:49:53,620
Right?

791
00:49:59,110 --> 00:50:05,879
Other other questions.

792
00:50:08,920 --> 00:50:11,220
Yeah.

793
00:50:14,350 --> 00:50:17,610
Sorry, the epsilon
greedy policy.

794
00:50:17,950 --> 00:50:19,969
Yes.

795
00:50:19,969 --> 00:50:23,420
So this is an algorithm

796
00:50:23,420 --> 00:50:26,389
that needs to tell you how
to play the game, right?

797
00:50:26,389 --> 00:50:27,920
So when we're doing

798
00:50:27,920 --> 00:50:29,750
the timber different
than our fixed policy,

799
00:50:29,750 --> 00:50:32,914
whenever you need
to go for a step,

800
00:50:32,914 --> 00:50:36,019
you just query the policy
which has given to you.

801
00:50:36,019 --> 00:50:38,795
And now we don't have a
policy to start with anymore,

802
00:50:38,795 --> 00:50:40,989
but we formulate our own policy

803
00:50:40,989 --> 00:50:43,769
according to the epsilon
greedy rule, right?

804
00:50:43,769 --> 00:50:46,335
Yeah, So every time just
looking at in the beginning,

805
00:50:46,335 --> 00:50:47,579
everything is zero,
for instance,

806
00:50:47,579 --> 00:50:49,395
that's your neutralization.

807
00:50:49,395 --> 00:50:51,870
And then just take
whatever, right?

808
00:50:51,870 --> 00:50:55,140
So you'd say probability
of the best one,

809
00:50:55,140 --> 00:50:56,790
the best one,
everything is the best.

810
00:50:56,790 --> 00:50:58,305
So just take an arbitrary one

811
00:50:58,305 --> 00:50:59,655
and then you are going to start

812
00:50:59,655 --> 00:51:02,024
accumulating
interesting numbers for

813
00:51:02,024 --> 00:51:04,125
Q values of all your actions.

814
00:51:04,125 --> 00:51:09,070
And yeah, just, just
go with that formula.

815
00:51:17,510 --> 00:51:20,864
Okay, So all the
pseudocode and stuff

816
00:51:20,864 --> 00:51:27,430
is It's in the slides.

817
00:51:31,610 --> 00:51:33,930
You know, that's,
that's the code

818
00:51:33,930 --> 00:51:37,815
for Q-learning and yeah,

819
00:51:37,815 --> 00:51:39,644
you should be able
to just implemented.

820
00:51:39,644 --> 00:51:44,534
And in the PA, basically,

821
00:51:44,534 --> 00:51:48,405
it first asks you to
do these and then you,

822
00:51:48,405 --> 00:51:49,844
once you implement Q-learning,

823
00:51:49,844 --> 00:51:52,470
you can let it do
Q-learning for some time.

824
00:51:52,470 --> 00:51:54,959
And then there's a
button called autoplay.

825
00:51:54,959 --> 00:51:58,229
Autoplay just takes
your current q-values

826
00:51:58,229 --> 00:52:01,005
and start taking
the greedy actions.

827
00:52:01,005 --> 00:52:04,380
If in the earliest stage
of your Q-learning,

828
00:52:04,380 --> 00:52:05,594
you probably don't have

829
00:52:05,594 --> 00:52:07,679
very good understanding
about the actions

830
00:52:07,679 --> 00:52:11,324
that as your
Q-learning stabilizes,

831
00:52:11,324 --> 00:52:14,160
meaning all these
value estimates get

832
00:52:14,160 --> 00:52:17,399
closer and closer to their
ground truce, correct value.

833
00:52:17,399 --> 00:52:19,935
When you hit the autoplay,

834
00:52:19,935 --> 00:52:21,389
you're basically playing with

835
00:52:21,389 --> 00:52:25,169
the optimal policy for
this version of blackjack.

836
00:52:25,169 --> 00:52:28,034
And then there's some kind of
win rate that you can see.

837
00:52:28,034 --> 00:52:30,495
What is the win rate
that you can get

838
00:52:30,495 --> 00:52:33,960
under the optimal policy that's
obtained from Q-learning.

839
00:52:33,960 --> 00:52:38,174
And that's how you
fully solve that.

840
00:52:38,174 --> 00:52:43,184
Okay? Alright. Okay.

841
00:52:43,184 --> 00:52:52,230
So now again,

842
00:52:52,230 --> 00:52:54,539
overall MDP and reinforce

843
00:52:54,539 --> 00:52:58,065
morning you have values and
policies to think about.

844
00:52:58,065 --> 00:52:59,699
Everything works out.

845
00:52:59,699 --> 00:53:02,850
Was these weird
formulas just because,

846
00:53:02,850 --> 00:53:06,120
you know, they're lying
all the questions.

847
00:53:06,120 --> 00:53:08,010
There's the structure of

848
00:53:08,010 --> 00:53:11,459
the value space and the
kind of policy space.

849
00:53:11,459 --> 00:53:12,810
And you know,
there's contraction

850
00:53:12,810 --> 00:53:14,609
Bellman equation, blah, blah.

851
00:53:14,609 --> 00:53:16,785
That happens.

852
00:53:16,785 --> 00:53:19,815
And as long as you're doing
things in the right way,

853
00:53:19,815 --> 00:53:21,150
they will always converge

854
00:53:21,150 --> 00:53:25,875
the best possible answer
for all these questions.

855
00:53:25,875 --> 00:53:30,360
And I don't want to
start the kind of

856
00:53:30,360 --> 00:53:32,595
deep reinforcement learning here

857
00:53:32,595 --> 00:53:36,944
because I think we're
going to come back to it.

858
00:53:36,944 --> 00:53:38,325
I mean, we won't really cover

859
00:53:38,325 --> 00:53:40,514
deep reinforcement
learning because

860
00:53:40,514 --> 00:53:42,615
it's just you saying,
Well, it's different.

861
00:53:42,615 --> 00:53:44,879
For asthma learning
allows you to do this.

862
00:53:44,879 --> 00:53:47,685
But really it's doing
the same algorithm,

863
00:53:47,685 --> 00:53:50,729
but throw in neural
networks there.

864
00:53:50,729 --> 00:53:51,225
Okay?

865
00:53:51,225 --> 00:53:53,370
So in deep
reinforcement learning,

866
00:53:53,370 --> 00:53:56,159
basically the idea is
right now in your PA,

867
00:53:56,159 --> 00:53:58,365
everything is a lookup table.

868
00:53:58,365 --> 00:54:00,899
But in general, you can use

869
00:54:00,899 --> 00:54:04,529
a neural network to
replace the lookup table.

870
00:54:04,529 --> 00:54:06,450
Okay, for those of
you who have done

871
00:54:06,450 --> 00:54:08,339
machine learning, you know,

872
00:54:08,339 --> 00:54:09,599
deep learning, you know,

873
00:54:09,599 --> 00:54:12,270
that's essentially a one-year
networks are doing, right?

874
00:54:12,270 --> 00:54:13,379
You give it a lot of inputs,

875
00:54:13,379 --> 00:54:14,610
that give it a lot
of outputs and

876
00:54:14,610 --> 00:54:16,185
tell it, memorize these.

877
00:54:16,185 --> 00:54:19,860
And then it's going to be
able to reproduce that

878
00:54:19,860 --> 00:54:22,035
for one thing and also kind of

879
00:54:22,035 --> 00:54:25,050
give you an interpolation
on new queries.

880
00:54:25,050 --> 00:54:27,885
So once you put
neural networks in

881
00:54:27,885 --> 00:54:30,989
the reinforced
learning structure,

882
00:54:30,989 --> 00:54:32,459
you can start dealing with as

883
00:54:32,459 --> 00:54:35,355
much louder States and
much larger action spaces.

884
00:54:35,355 --> 00:54:36,555
You can even deal with

885
00:54:36,555 --> 00:54:39,554
infinite continuous
data and action spaces,

886
00:54:39,554 --> 00:54:41,940
which you wouldn't need
to do this, right?

887
00:54:41,940 --> 00:54:43,499
Because you have all these

888
00:54:43,499 --> 00:54:45,659
physical parameters
and they are in

889
00:54:45,659 --> 00:54:47,640
principle continuous values that

890
00:54:47,640 --> 00:54:50,070
you cannot have a simple
lookup table for.

891
00:54:50,070 --> 00:54:52,680
Alright, so we're not going

892
00:54:52,680 --> 00:54:56,189
to talk about that
too much about later.

893
00:54:56,189 --> 00:54:58,545
I will still try to
come back to it.

894
00:54:58,545 --> 00:55:00,405
By the way, this is
just saying, you know,

895
00:55:00,405 --> 00:55:03,944
your values and your policies
can all be neural networks.

896
00:55:03,944 --> 00:55:05,589
Alright?

897
00:55:05,840 --> 00:55:08,099
Yeah, I have some
slides on that,

898
00:55:08,099 --> 00:55:10,890
but I want to mention
this a little bit because

899
00:55:10,890 --> 00:55:16,094
the real goal for me is to
don't worry about these.

900
00:55:16,094 --> 00:55:19,844
None of these is
part of the class.

901
00:55:19,844 --> 00:55:21,825
I'm sorry.

902
00:55:21,825 --> 00:55:23,954
I'm okay.

903
00:55:23,954 --> 00:55:25,650
I don't have the AlphaGo slide,

904
00:55:25,650 --> 00:55:27,539
but I want to mention it

905
00:55:27,539 --> 00:55:29,100
because in the end I want

906
00:55:29,100 --> 00:55:30,809
to fully explain AlphaGo to you.

907
00:55:30,809 --> 00:55:33,195
And right now we
have half story,

908
00:55:33,195 --> 00:55:35,235
which is reinforcement learning.

909
00:55:35,235 --> 00:55:38,774
In AlphaGo, we're going to use

910
00:55:38,774 --> 00:55:44,445
q values from reinforced
learning to inform.

911
00:55:44,445 --> 00:55:46,380
The next algorithm we're
going to talk about,

912
00:55:46,380 --> 00:55:48,224
which is Monte
Carlo tree search.

913
00:55:48,224 --> 00:55:48,974
Okay?

914
00:55:48,974 --> 00:55:52,454
So after all these
three weeks in AlphaGo,

915
00:55:52,454 --> 00:55:54,120
this is the only thing

916
00:55:54,120 --> 00:55:56,820
that you're eventually
going to use.

917
00:55:56,820 --> 00:56:00,839
Which is actually the
only thing you would need

918
00:56:00,839 --> 00:56:04,364
because playing a
Go game like that,

919
00:56:04,364 --> 00:56:06,374
the only question
is, at every state,

920
00:56:06,374 --> 00:56:08,984
what are your estimation

921
00:56:08,984 --> 00:56:10,769
of the quality of
different actions?

922
00:56:10,769 --> 00:56:12,914
And you'd probably just
go with the best one.

923
00:56:12,914 --> 00:56:16,829
But reinforce learning
itself is usually not

924
00:56:16,829 --> 00:56:20,460
enough for problems with
large enough spaces.

925
00:56:20,460 --> 00:56:21,975
Because as you see,

926
00:56:21,975 --> 00:56:24,270
the correct understanding of

927
00:56:24,270 --> 00:56:28,005
these q-value is all dependent
on your visitation, right?

928
00:56:28,005 --> 00:56:29,549
So if you only train things

929
00:56:29,549 --> 00:56:31,349
with reinforcement
learning and then you play

930
00:56:31,349 --> 00:56:35,279
a real game was really
smart people against you.

931
00:56:35,279 --> 00:56:37,755
If it's only coming from data.

932
00:56:37,755 --> 00:56:40,019
First of all, we know the
space is too large for you

933
00:56:40,019 --> 00:56:42,829
to actually reliably
estimate everything.

934
00:56:42,829 --> 00:56:44,534
At the same time,

935
00:56:44,534 --> 00:56:48,165
your adversary can
lead you to a state

936
00:56:48,165 --> 00:56:52,739
that is just not sin so
much in your training data.

937
00:56:52,739 --> 00:56:54,899
So learning itself always has

938
00:56:54,899 --> 00:56:57,449
the problem of things
that you haven't

939
00:56:57,449 --> 00:57:02,265
seen enough times in
training than in using it.

940
00:57:02,265 --> 00:57:05,139
You really don't
know much about it.

941
00:57:05,240 --> 00:57:07,634
And because of that,

942
00:57:07,634 --> 00:57:11,115
we actually need
another new version

943
00:57:11,115 --> 00:57:13,770
of the minimax
search, which now,

944
00:57:13,770 --> 00:57:16,950
given all these new understanding
about probabilities,

945
00:57:16,950 --> 00:57:18,885
about taking long Carlo samples,

946
00:57:18,885 --> 00:57:20,114
we can actually have

947
00:57:20,114 --> 00:57:22,740
a new advanced version of
Monte Carlo tree search,

948
00:57:22,740 --> 00:57:26,610
which will be the
other part of AlphaGo.

949
00:57:26,610 --> 00:57:28,725
So in the end,
when we get there,

950
00:57:28,725 --> 00:57:33,210
AlphaGo is simply Monte Carlo
Tree Search plus q values.

951
00:57:33,210 --> 00:57:36,180
We'll get there in
1.5 weeks, I think.

952
00:57:36,180 --> 00:57:41,350
Okay, So now let just
start the second part.

953
00:57:53,870 --> 00:57:56,139
Alright.

954
00:57:56,720 --> 00:57:58,245
Okay.

955
00:57:58,245 --> 00:58:02,999
So coming back to the second
week, there were no problem.

956
00:58:02,999 --> 00:58:07,754
He's just a huge tree that
we somehow come up with,

957
00:58:07,754 --> 00:58:09,749
evaluation functions
and all that.

958
00:58:09,749 --> 00:58:15,359
And you know, if you do this
for 2048 is sort of okay.

959
00:58:15,359 --> 00:58:16,770
Doing this for Chez,

960
00:58:16,770 --> 00:58:19,499
you can engineer things and
get reasonable answers.

961
00:58:19,499 --> 00:58:20,969
But for a very long time,

962
00:58:20,969 --> 00:58:22,619
people didn't know
how to do this for

963
00:58:22,619 --> 00:58:25,995
a real game and not bill for
large enough game like Go.

964
00:58:25,995 --> 00:58:29,520
And go is compare it
to real problems.

965
00:58:29,520 --> 00:58:31,424
Go is also still small.

966
00:58:31,424 --> 00:58:33,869
But if we want to scale

967
00:58:33,869 --> 00:58:37,484
up all these tree
search algorithms,

968
00:58:37,484 --> 00:58:39,729
we need new ideas.

969
00:58:42,170 --> 00:58:46,079
New ideas in AI in the past,

970
00:58:46,079 --> 00:58:49,605
say 30 years, always came from.

971
00:58:49,605 --> 00:58:52,139
Kinda Theta and probabilities.

972
00:58:52,139 --> 00:58:59,175
So minimax algorithm
itself was from 1960s.

973
00:58:59,175 --> 00:59:02,759
I think that was a
time when a star and

974
00:59:02,759 --> 00:59:04,950
Minimax Search and alpha-beta

975
00:59:04,950 --> 00:59:06,465
pruning and all that came out.

976
00:59:06,465 --> 00:59:09,299
Because when people started

977
00:59:09,299 --> 00:59:12,254
thinking about these
classical algorithm,

978
00:59:12,254 --> 00:59:13,575
that's around time when

979
00:59:13,575 --> 00:59:16,064
most of the clever
ideas already came out.

980
00:59:16,064 --> 00:59:17,954
And then for a long time,

981
00:59:17,954 --> 00:59:20,279
there's not much progress that
most of the progress is in

982
00:59:20,279 --> 00:59:21,734
the engineering or the value

983
00:59:21,734 --> 00:59:23,789
evaluation function
and blah, blah, right?

984
00:59:23,789 --> 00:59:26,684
And then starting from,
let's say the 80s,

985
00:59:26,684 --> 00:59:30,179
we realized that learning
and data are really part of

986
00:59:30,179 --> 00:59:34,650
the important piece
of intelligence.

987
00:59:34,650 --> 00:59:39,075
And naturally people start
thinking about, okay,

988
00:59:39,075 --> 00:59:41,880
you have these huge trees
and you didn't know how

989
00:59:41,880 --> 00:59:45,945
to predict things
in this big space.

990
00:59:45,945 --> 00:59:49,514
What if we try sampling?

991
00:59:49,514 --> 00:59:50,805
Right?

992
00:59:50,805 --> 00:59:54,060
In Rufus morning sampling
worked really well.

993
00:59:54,060 --> 00:59:55,815
Because from the sampling,

994
00:59:55,815 --> 00:59:59,430
you actually figure out
algorithms that allow you to get

995
00:59:59,430 --> 01:00:01,455
the optimal answers without

996
01:00:01,455 --> 01:00:05,579
any knowledge about the
underlying environment, right?

997
01:00:05,579 --> 01:00:07,665
So then people will
start thinking, Okay,

998
01:00:07,665 --> 01:00:11,385
what if we try sampling
in terms of tree search?

999
01:00:11,385 --> 01:00:16,109
So the idea is the
ground-truth tree is

1000
01:00:16,109 --> 01:00:20,414
always going to be
large and you know,

1001
01:00:20,414 --> 01:00:22,964
but we can always play games,

1002
01:00:22,964 --> 01:00:24,779
play trajectories, which means

1003
01:00:24,779 --> 01:00:27,075
enrolling trajectories
in this tree.

1004
01:00:27,075 --> 01:00:29,385
And can we somehow get

1005
01:00:29,385 --> 01:00:35,025
evaluation functions out of
our samples of the tree?

1006
01:00:35,025 --> 01:00:38,710
Alright, so that's the setup.

1007
01:00:40,400 --> 01:00:43,485
This is your next PA,

1008
01:00:43,485 --> 01:00:46,770
which is a game that looks,

1009
01:00:46,770 --> 01:00:50,055
I mean, it sounds goal board
about as much simpler game.

1010
01:00:50,055 --> 01:00:52,950
Again, I don't
understand goal at all.

1011
01:00:52,950 --> 01:00:56,685
So this is just kinda come up
for Connect five, I think.

1012
01:00:56,685 --> 01:01:01,109
And then showing is

1013
01:01:01,109 --> 01:01:03,135
kinda Monte-Carlo
tree search working.

1014
01:01:03,135 --> 01:01:05,040
Basically figuring out what are

1015
01:01:05,040 --> 01:01:07,874
the options that are
the most interesting,

1016
01:01:07,874 --> 01:01:10,694
most promising for
your next step?

1017
01:01:10,694 --> 01:01:13,365
And I think I forgot
which one is AI.

1018
01:01:13,365 --> 01:01:16,380
But when you do the algorithm,

1019
01:01:16,380 --> 01:01:19,960
right, it can play a
pretty competitive game.

1020
01:01:20,570 --> 01:01:25,185
Usually I run a
small competition

1021
01:01:25,185 --> 01:01:28,170
at the end of the quarter.

1022
01:01:28,170 --> 01:01:31,020
No extra credit
associated with it.

1023
01:01:31,020 --> 01:01:34,514
I don't want you to get all
competitive about that.

1024
01:01:34,514 --> 01:01:36,870
But you know, I usually get

1025
01:01:36,870 --> 01:01:38,760
some pretty interesting
submissions

1026
01:01:38,760 --> 01:01:40,799
and we're going to play out some

1027
01:01:40,799 --> 01:01:42,090
of this round solutions and

1028
01:01:42,090 --> 01:01:43,590
see that they're going to fully

1029
01:01:43,590 --> 01:01:47,400
cover the whole board
eventually in most cases.

1030
01:01:47,400 --> 01:01:48,629
Anyway.

1031
01:01:48,629 --> 01:01:53,880
So the idea is actually

1032
01:01:53,880 --> 01:01:56,519
to understand about what

1033
01:01:56,519 --> 01:02:00,149
happens when we take Monte
Carlo's samples more.

1034
01:02:00,149 --> 01:02:03,539
Monte Carlo methods is
a very general method.

1035
01:02:03,539 --> 01:02:04,860
You can't use it.

1036
01:02:04,860 --> 01:02:06,839
So first of all, I'm in
current method is just saying,

1037
01:02:06,839 --> 01:02:08,849
suppose you have
something that's

1038
01:02:08,849 --> 01:02:12,495
a ground truth that you
don't really know, right?

1039
01:02:12,495 --> 01:02:18,255
Then you have access to IID
samples of this ground truce.

1040
01:02:18,255 --> 01:02:20,969
Then from the samples
you can actually

1041
01:02:20,969 --> 01:02:24,464
understand a lot about
the ground truths.

1042
01:02:24,464 --> 01:02:28,770
For instance, the most
classical example, if it is,

1043
01:02:28,770 --> 01:02:32,880
I'm gonna give you a very
simple algorithm of calculating

1044
01:02:32,880 --> 01:02:37,229
pi without
understanding anything

1045
01:02:37,229 --> 01:02:40,845
about calculus, for instance.

1046
01:02:40,845 --> 01:02:42,179
Okay?

1047
01:02:42,179 --> 01:02:47,130
So the algorithm that I
propose is like this.

1048
01:02:47,130 --> 01:02:57,969
Use your computer or whatever
to draw me this shape.

1049
01:03:02,600 --> 01:03:04,215
Okay?

1050
01:03:04,215 --> 01:03:12,539
So this part is one quarter
of the circle or whatever.

1051
01:03:12,539 --> 01:03:16,529
And dance a square, right?

1052
01:03:16,529 --> 01:03:18,869
So as 11 on both sides.

1053
01:03:18,869 --> 01:03:25,505
And this is, and then
what I'm gonna do

1054
01:03:25,505 --> 01:03:34,159
is draw random samples from
the square, which you can do.

1055
01:03:34,159 --> 01:03:36,185
I just uniformly
sample from the x

1056
01:03:36,185 --> 01:03:38,704
uniformly assembled
around the y, right?

1057
01:03:38,704 --> 01:03:41,010
I just draw those samples.

1058
01:03:45,040 --> 01:03:47,340
Okay?

1059
01:03:48,070 --> 01:03:50,645
And then for each sample,

1060
01:03:50,645 --> 01:03:54,365
I can easily check whether
given the x, y coordinates,

1061
01:03:54,365 --> 01:03:58,145
whether it's inside the,

1062
01:03:58,145 --> 01:04:04,110
you know, the, the
pie or it's outside.

1063
01:04:07,340 --> 01:04:11,850
Right, just from San
Jose, I can do that.

1064
01:04:11,850 --> 01:04:18,299
And then I look at

1065
01:04:18,299 --> 01:04:22,200
the ratio between the number of

1066
01:04:22,200 --> 01:04:26,939
red samples with
the total number

1067
01:04:26,939 --> 01:04:29,439
of points I have sampled.

1068
01:04:32,870 --> 01:04:35,230
Okay?

1069
01:04:36,290 --> 01:04:39,989
I take Monte Carlo samples of

1070
01:04:39,989 --> 01:04:43,514
something and I
take this average.

1071
01:04:43,514 --> 01:04:46,989
What would this
average converge to?

1072
01:04:48,230 --> 01:04:51,000
It will converge
to the racial of

1073
01:04:51,000 --> 01:04:55,209
this area over the square.

1074
01:04:57,770 --> 01:05:01,240
So it will converge to what?

1075
01:05:05,480 --> 01:05:09,450
Okay, I don't need to
understand anything about,

1076
01:05:09,450 --> 01:05:11,190
you know, how pie is represented

1077
01:05:11,190 --> 01:05:13,214
as an infinite series
of rule of law.

1078
01:05:13,214 --> 01:05:15,029
I will just do this experiment.

1079
01:05:15,029 --> 01:05:17,729
I be a computer scientist.

1080
01:05:17,729 --> 01:05:19,844
Alright, sample.

1081
01:05:19,844 --> 01:05:21,360
For each sample I check.

1082
01:05:21,360 --> 01:05:23,264
Then I collect them all.

1083
01:05:23,264 --> 01:05:25,095
In the end, I get this number,

1084
01:05:25,095 --> 01:05:28,710
I multiply it by four, I get pi.

1085
01:05:28,710 --> 01:05:30,104
Alright?

1086
01:05:30,104 --> 01:05:33,400
So this is the power of
Monte Carlo methods.

1087
01:05:35,750 --> 01:05:37,409
Okay?

1088
01:05:37,409 --> 01:05:40,125
So from samples,
you can understand

1089
01:05:40,125 --> 01:05:43,574
a lot about things that aren't
inherently complicated,

1090
01:05:43,574 --> 01:05:45,540
which is what you're seeing in

1091
01:05:45,540 --> 01:05:47,535
machine learning these
days as well, right?

1092
01:05:47,535 --> 01:05:50,639
So we don't really know what,

1093
01:05:50,639 --> 01:05:52,799
you know, what pictures are,

1094
01:05:52,799 --> 01:05:54,270
Caswell pictures of dogs.

1095
01:05:54,270 --> 01:05:56,474
And but if you try
enough samples,

1096
01:05:56,474 --> 01:05:57,959
you can learn a model that

1097
01:05:57,959 --> 01:06:00,779
sufficiently
differentiate the two.

1098
01:06:00,779 --> 01:06:01,650
Alright?

1099
01:06:01,650 --> 01:06:04,470
So there's this thing,

1100
01:06:04,470 --> 01:06:06,910
I don't know whether you get it.

1101
01:06:10,250 --> 01:06:12,585
By discounting the arrows

1102
01:06:12,585 --> 01:06:14,849
on this thing, you
can calculate pi.

1103
01:06:14,849 --> 01:06:16,094
Anyway.

1104
01:06:16,094 --> 01:06:19,784
So we're gonna do that
for our tree search.

1105
01:06:19,784 --> 01:06:24,015
So the main question that
we have in Minimax Search

1106
01:06:24,015 --> 01:06:33,400
is you have the big trees

1107
01:06:35,630 --> 01:06:40,200
that you potentially
need to grow, right?

1108
01:06:40,200 --> 01:06:41,894
But you can say,

1109
01:06:41,894 --> 01:06:45,705
let me grow the tree based
on kind of my Q values,

1110
01:06:45,705 --> 01:06:48,479
which you kind of do
Monte Carlo estimates on.

1111
01:06:48,479 --> 01:06:51,719
The idea is, let's just
try out actions and see

1112
01:06:51,719 --> 01:06:55,869
whether they lead
me to good path.

1113
01:06:55,970 --> 01:06:58,485
So I can start with

1114
01:06:58,485 --> 01:07:05,385
my root node and try
out different actions.

1115
01:07:05,385 --> 01:07:09,195
Then from those, I can somehow

1116
01:07:09,195 --> 01:07:11,009
determine a way of choosing

1117
01:07:11,009 --> 01:07:13,365
actions and keep
going down the tree.

1118
01:07:13,365 --> 01:07:16,740
And then from these
Monte Carlo samples

1119
01:07:16,740 --> 01:07:19,304
of the different trees,

1120
01:07:19,304 --> 01:07:25,455
I can inform about the
future growth of the tree.

1121
01:07:25,455 --> 01:07:27,779
Again, this is very similar
to Q-learning, right?

1122
01:07:27,779 --> 01:07:29,939
So now that you have
that idea in mind,

1123
01:07:29,939 --> 01:07:31,289
this is very similar,

1124
01:07:31,289 --> 01:07:35,115
but this is about growing
the minimax tree eventually,

1125
01:07:35,115 --> 01:07:37,859
which doesn't really have
probabilities in it.

1126
01:07:37,859 --> 01:07:38,684
Okay?

1127
01:07:38,684 --> 01:07:40,890
We're really just
talking about growing

1128
01:07:40,890 --> 01:07:43,395
the ground choose minimax tree.

1129
01:07:43,395 --> 01:07:45,554
There's no probability
this is not an MDP,

1130
01:07:45,554 --> 01:07:48,270
but we're going to use samples

1131
01:07:48,270 --> 01:07:53,414
to approximate the right
evaluation functions.

1132
01:07:53,414 --> 01:07:56,505
So used to something that
has probabilities in there

1133
01:07:56,505 --> 01:08:00,330
to eventually grow the tree
in the most interesting part.

1134
01:08:00,330 --> 01:08:02,220
So eventually what
we're looking for

1135
01:08:02,220 --> 01:08:05,320
is an algorithm that,

1136
01:08:11,350 --> 01:08:14,270
remember, we talked about

1137
01:08:14,270 --> 01:08:19,649
minimax trees is something
huge, like that.

1138
01:08:20,680 --> 01:08:22,430
Whatever, right?

1139
01:08:22,430 --> 01:08:24,350
So it's a huge thing.

1140
01:08:24,350 --> 01:08:28,400
What I want to achieve
with this new way of

1141
01:08:28,400 --> 01:08:33,899
growing the tree is to
only grow the parts.

1142
01:08:35,500 --> 01:08:41,314
That leads to interesting
games and ignore everything

1143
01:08:41,314 --> 01:08:44,119
else that will not be

1144
01:08:44,119 --> 01:08:47,829
useful for understanding
my optimal actions.

1145
01:08:47,829 --> 01:08:48,689
Okay?

1146
01:08:48,689 --> 01:08:50,595
So this is the
ground-truth tree.

1147
01:08:50,595 --> 01:08:53,489
I want to use sampling to

1148
01:08:53,489 --> 01:08:56,040
generate something like this

1149
01:08:56,040 --> 01:08:58,979
in the data structure
eventually.

1150
01:08:58,979 --> 01:09:02,384
Okay, and unbalanced growth of

1151
01:09:02,384 --> 01:09:05,835
the ground-truth
huge minimax tree.

1152
01:09:05,835 --> 01:09:08,939
And now you see why
I wasn't emphasizing

1153
01:09:08,939 --> 01:09:11,460
so much on these vertical
cutoff or horizontal

1154
01:09:11,460 --> 01:09:13,364
cut off because all those are

1155
01:09:13,364 --> 01:09:16,544
kind of '60s and
'70s techniques.

1156
01:09:16,544 --> 01:09:19,365
Now that we have
better understanding

1157
01:09:19,365 --> 01:09:22,920
about samples and distributions,

1158
01:09:22,920 --> 01:09:25,560
this is actually
the best way to cut

1159
01:09:25,560 --> 01:09:29,475
out useful parts of the tree.

1160
01:09:29,475 --> 01:09:33,730
So we're going to cover an
algorithm that does this.

1161
01:09:34,850 --> 01:09:40,080
So let's imagine the
design of this algorithm.

1162
01:09:40,080 --> 01:09:42,689
You are going to start
from the root node.

1163
01:09:42,689 --> 01:09:47,024
And then at least you should
try out some actions, right?

1164
01:09:47,024 --> 01:09:51,060
So even if you just
randomly sample actions,

1165
01:09:51,060 --> 01:09:54,719
or we're going to see samples
of trajectory that goes

1166
01:09:54,719 --> 01:09:58,304
from that empty board to
the end of the board.

1167
01:09:58,304 --> 01:10:02,444
So you have one trajectory
down the tree, right?

1168
01:10:02,444 --> 01:10:04,319
And based on whether
that trajectory

1169
01:10:04,319 --> 01:10:06,315
is winning or losing in the end,

1170
01:10:06,315 --> 01:10:08,264
you can sort of keep track of,

1171
01:10:08,264 --> 01:10:11,879
is this action a according
to your current sample,

1172
01:10:11,879 --> 01:10:13,859
whether it's good or bad action.

1173
01:10:13,859 --> 01:10:15,045
Alright?

1174
01:10:15,045 --> 01:10:18,014
And then you can try
something else and see

1175
01:10:18,014 --> 01:10:21,449
more samples of what

1176
01:10:21,449 --> 01:10:25,360
these different actions give
you a different futures.

1177
01:10:27,110 --> 01:10:31,185
But then the core
question to ask here is,

1178
01:10:31,185 --> 01:10:33,944
after doing this for awhile,

1179
01:10:33,944 --> 01:10:37,090
how do you actually know

1180
01:10:37,790 --> 01:10:43,695
which part of the tree you
should actually grow? Right?

1181
01:10:43,695 --> 01:10:44,969
You only thing that
you can keep track

1182
01:10:44,969 --> 01:10:46,589
of is you try some actions,

1183
01:10:46,589 --> 01:10:50,219
you sample their future
path and you keep track of,

1184
01:10:50,219 --> 01:10:52,785
okay, auto my two samples.

1185
01:10:52,785 --> 01:10:55,470
This action had one win

1186
01:10:55,470 --> 01:10:57,749
and the other action
had two wins.

1187
01:10:57,749 --> 01:10:58,859
Right?

1188
01:10:58,859 --> 01:11:00,285
Then in the future,

1189
01:11:00,285 --> 01:11:02,114
do I grow the,

1190
01:11:02,114 --> 01:11:04,019
the node that has one

1191
01:11:04,019 --> 01:11:07,689
when or do I grow the
node that has two wings?

1192
01:11:08,390 --> 01:11:10,875
Of course, if you're a greedy,

1193
01:11:10,875 --> 01:11:15,164
you will go with the
two when parked, right?

1194
01:11:15,164 --> 01:11:18,240
But everything is stochastic.

1195
01:11:18,240 --> 01:11:20,294
You really don't know whether

1196
01:11:20,294 --> 01:11:23,580
the current best option

1197
01:11:23,580 --> 01:11:28,649
is actually wasting your
time for the future.

1198
01:11:28,649 --> 01:11:30,795
So this is why I say,

1199
01:11:30,795 --> 01:11:32,910
we're going to look at

1200
01:11:32,910 --> 01:11:35,099
this exploration
versus exploitation

1201
01:11:35,099 --> 01:11:37,154
question in detail now.

1202
01:11:37,154 --> 01:11:40,185
Because it reinforced
learning, we say,

1203
01:11:40,185 --> 01:11:41,369
as long as we leave out

1204
01:11:41,369 --> 01:11:44,320
some probabilities for
visiting, everything is fine.

1205
01:11:44,320 --> 01:11:45,765
Everything is going to converge.

1206
01:11:45,765 --> 01:11:48,104
Contraction Boulevard,
you just say that.

1207
01:11:48,104 --> 01:11:51,344
But now if we're
doing Minimax Search,

1208
01:11:51,344 --> 01:11:53,429
imagine this is what
you're going do.

1209
01:11:53,429 --> 01:11:56,084
It's going to be doing
when you are really

1210
01:11:56,084 --> 01:11:57,479
implementing AI it as

1211
01:11:57,479 --> 01:11:58,920
putting in time for
a playing chess,

1212
01:11:58,920 --> 01:12:01,320
playing goal, time
actually matters.

1213
01:12:01,320 --> 01:12:04,439
So you have probably
a fixed budget,

1214
01:12:04,439 --> 01:12:08,054
maybe 1,000 simulations, right?

1215
01:12:08,054 --> 01:12:12,359
Then, how do you try your
best to grow the best part of

1216
01:12:12,359 --> 01:12:16,214
the tree and not
waste your budgets

1217
01:12:16,214 --> 01:12:20,504
on things that you just
have to try a little bit.

1218
01:12:20,504 --> 01:12:23,579
So the trade-off
between exploration

1219
01:12:23,579 --> 01:12:26,954
versus exploitation
is the most critical,

1220
01:12:26,954 --> 01:12:30,089
critical thing here this morning

1221
01:12:30,089 --> 01:12:32,909
because we don't care
about it just ran forever.

1222
01:12:32,909 --> 01:12:34,919
It's the values that

1223
01:12:34,919 --> 01:12:37,035
we tried to figure out the
best law Miller's floor.

1224
01:12:37,035 --> 01:12:40,560
But now we need to think about

1225
01:12:40,560 --> 01:12:43,289
carefully what the trade-off

1226
01:12:43,289 --> 01:12:45,360
between exploration
and exploitation.

1227
01:12:45,360 --> 01:12:46,435
This.

1228
01:12:46,435 --> 01:12:48,034
Okay?

1229
01:12:48,034 --> 01:12:52,340
So now let me give you the
simplest version of that.

1230
01:12:52,340 --> 01:12:55,954
I'm going to come back to that.

1231
01:12:55,954 --> 01:12:57,620
There's a multi-arm bandits,

1232
01:12:57,620 --> 01:12:59,315
but let me give you the simplest

1233
01:12:59,315 --> 01:13:01,205
game to think about this.

1234
01:13:01,205 --> 01:13:03,185
You have two coins,

1235
01:13:03,185 --> 01:13:05,450
while someone has two coins.

1236
01:13:05,450 --> 01:13:06,815
Alright?

1237
01:13:06,815 --> 01:13:09,689
So one coin has,

1238
01:13:19,210 --> 01:13:23,495
so you have two different
coins, blue and red.

1239
01:13:23,495 --> 01:13:28,264
Let's say the ground
truth of them,

1240
01:13:28,264 --> 01:13:31,155
you don t know, but let's
just give that to you.

1241
01:13:31,155 --> 01:13:38,549
For one coin, you
get 0.8 of one side.

1242
01:13:38,549 --> 01:13:40,634
And for the other coin.

1243
01:13:40,634 --> 01:13:45,010
These are not fair
coins apparently.

1244
01:13:46,790 --> 01:13:50,709
But they both have
a distribution.

1245
01:13:51,680 --> 01:13:54,345
0.2, okay?

1246
01:13:54,345 --> 01:13:58,814
And let's say you're only
allowed to bet on pad,

1247
01:13:58,814 --> 01:14:00,974
only had gives you $1.

1248
01:14:00,974 --> 01:14:03,044
Right?

1249
01:14:03,044 --> 01:14:06,374
This is the, this is
what the dealer knows.

1250
01:14:06,374 --> 01:14:08,010
This is not something
that you know.

1251
01:14:08,010 --> 01:14:08,655
Okay.

1252
01:14:08,655 --> 01:14:11,490
I'll, you can see
is these two coins.

1253
01:14:11,490 --> 01:14:14,594
And each time it's round,

1254
01:14:14,594 --> 01:14:16,424
all that you can choose is,

1255
01:14:16,424 --> 01:14:19,005
which coin do you
put your money on?

1256
01:14:19,005 --> 01:14:21,689
Again, your money is
only put on this, right?

1257
01:14:21,689 --> 01:14:26,084
So I say, I write two mu is
basically if you choose this,

1258
01:14:26,084 --> 01:14:29,519
then you have 80% of
chance or you have

1259
01:14:29,519 --> 01:14:34,844
0.8 being your mu, one
expectation, right?

1260
01:14:34,844 --> 01:14:42,009
Or you have 0.2
being your mutate.

1261
01:14:42,620 --> 01:14:45,720
Every time the only choice,

1262
01:14:45,720 --> 01:14:48,450
the only decision
you make is you

1263
01:14:48,450 --> 01:14:52,209
choose the blue coin or do
you choose the red coin?

1264
01:14:53,890 --> 01:14:57,605
Again, without knowing
this information.

1265
01:14:57,605 --> 01:14:59,705
So this is in general called

1266
01:14:59,705 --> 01:15:01,999
the multi-arm bandit problem.

1267
01:15:01,999 --> 01:15:04,474
And someone came out,

1268
01:15:04,474 --> 01:15:06,589
came out with their
room with the name.

1269
01:15:06,589 --> 01:15:08,930
Basically says,
imagine you go to

1270
01:15:08,930 --> 01:15:10,309
the casino again and now you're

1271
01:15:10,309 --> 01:15:11,719
interested in the slot machines.

1272
01:15:11,719 --> 01:15:12,245
Okay?

1273
01:15:12,245 --> 01:15:13,730
So Islam machine that has

1274
01:15:13,730 --> 01:15:16,309
a fixed distribution that
you don't know about.

1275
01:15:16,309 --> 01:15:21,034
The machines now the the
the casino knows, right?

1276
01:15:21,034 --> 01:15:23,855
And you have fixed
amount of money

1277
01:15:23,855 --> 01:15:27,199
and you can just
add every round,

1278
01:15:27,199 --> 01:15:31,594
choose one of the
machines to play, right?

1279
01:15:31,594 --> 01:15:33,305
Suppose you only
have two machines.

1280
01:15:33,305 --> 01:15:34,820
One is good, one is bad,

1281
01:15:34,820 --> 01:15:36,429
but you don't know about that.

1282
01:15:36,429 --> 01:15:45,074
Then how do you
schedule your place?

1283
01:15:45,074 --> 01:15:46,559
Right?

1284
01:15:46,559 --> 01:15:51,839
So again, it's the
core abstraction

1285
01:15:51,839 --> 01:15:55,245
of the question that exploration
versus exploitation.

1286
01:15:55,245 --> 01:15:59,564
Maybe you have chosen this once.

1287
01:15:59,564 --> 01:16:03,344
Your first choice is the
red coin, Frances Wright.

1288
01:16:03,344 --> 01:16:06,420
And there's 20% of trans,

1289
01:16:06,420 --> 01:16:08,829
you actually get money from it.

1290
01:16:08,840 --> 01:16:12,674
So maybe you, the first step,

1291
01:16:12,674 --> 01:16:14,920
you see rare coin.

1292
01:16:16,340 --> 01:16:18,569
And it's good.

1293
01:16:18,569 --> 01:16:19,754
Right?

1294
01:16:19,754 --> 01:16:22,620
Then question is,
what do you do next?

1295
01:16:22,620 --> 01:16:28,994
If you're greedy, you're going
to play this again, right?

1296
01:16:28,994 --> 01:16:31,439
And if that still is,

1297
01:16:31,439 --> 01:16:34,574
well, then you're going
to keep playing that.

1298
01:16:34,574 --> 01:16:35,310
Okay?

1299
01:16:35,310 --> 01:16:37,199
So that's greedy.

1300
01:16:37,199 --> 01:16:43,230
Or if you just play them
kind of in a balanced way.

1301
01:16:43,230 --> 01:16:45,374
Let's say you're a
conservative person.

1302
01:16:45,374 --> 01:16:48,869
You played this once and
then you play the other.

1303
01:16:48,869 --> 01:16:50,714
And then you play this once,

1304
01:16:50,714 --> 01:16:52,469
and then you play the other.

1305
01:16:52,469 --> 01:16:56,849
You can allocate 50% of
chance on both coins.

1306
01:16:56,849 --> 01:16:58,709
And then in the end,

1307
01:16:58,709 --> 01:17:03,070
you're gonna get
something, right?

1308
01:17:03,740 --> 01:17:09,749
Or there's the epsilon
greedy strategy.

1309
01:17:09,749 --> 01:17:13,599
What would that be in this case?

1310
01:17:18,080 --> 01:17:20,235
Well, first give me something,

1311
01:17:20,235 --> 01:17:22,230
you know, how, how
would you play this?

1312
01:17:22,230 --> 01:17:24,194
Let's not get to
epsilon greedy yet.

1313
01:17:24,194 --> 01:17:34,275
How would you play
it? By the way,

1314
01:17:34,275 --> 01:17:36,060
when I say coins here,
really in the end,

1315
01:17:36,060 --> 01:17:39,809
they're going to be mapped to
choices I put on the tree.

1316
01:17:39,809 --> 01:17:41,385
But for now, let's
just say quiz.

1317
01:17:41,385 --> 01:17:42,670
Go ahead.

1318
01:17:46,730 --> 01:17:48,164
All right.

1319
01:17:48,164 --> 01:17:50,130
So let's say the red

1320
01:17:50,130 --> 01:17:56,099
one fairly give them
a number of times.

1321
01:17:56,099 --> 01:17:58,229
And then the blue

1322
01:17:58,229 --> 01:18:02,115
one fairly give them
a number of choices.

1323
01:18:02,115 --> 01:18:07,875
And then what maximize, right?

1324
01:18:07,875 --> 01:18:10,019
So then the, you know,

1325
01:18:10,019 --> 01:18:20,429
whatever that maximize
this red or blue.

1326
01:18:20,429 --> 01:18:22,870
In the future.

1327
01:18:24,410 --> 01:18:30,135
This is a strategy called
Explore than commit.

1328
01:18:30,135 --> 01:18:31,094
Okay?

1329
01:18:31,094 --> 01:18:34,080
So you first explore
all your options for

1330
01:18:34,080 --> 01:18:38,114
a fixed amount of times.

1331
01:18:38,114 --> 01:18:40,605
And then from that,

1332
01:18:40,605 --> 01:18:42,105
you believed that you have

1333
01:18:42,105 --> 01:18:44,670
acquired enough
information about the

1334
01:18:44,670 --> 01:18:46,139
two so that you can make

1335
01:18:46,139 --> 01:18:49,049
the comparison and go
with the better one.

1336
01:18:49,049 --> 01:18:50,354
From there.

1337
01:18:50,354 --> 01:18:55,119
Sounds like this is
better than this or this.

1338
01:18:55,880 --> 01:18:59,909
Precisely why we
say this is better,

1339
01:18:59,909 --> 01:19:04,830
requires a bunch of formulas.

1340
01:19:04,830 --> 01:19:08,999
We won't talk about it,
but you're going to enjoy

1341
01:19:08,999 --> 01:19:15,449
seeing those things
coming in next week.

1342
01:19:15,449 --> 01:19:16,155
Okay?

1343
01:19:16,155 --> 01:19:20,669
So then eventually, this
is not the optimal,

1344
01:19:20,669 --> 01:19:22,199
this is not the
optimal strategy.

1345
01:19:22,199 --> 01:19:24,569
By the way, we're gonna give you

1346
01:19:24,569 --> 01:19:27,435
something that's more
optimal than this provably.

1347
01:19:27,435 --> 01:19:30,389
And that's what you need to
do a Monte Carlo tree search.

1348
01:19:30,389 --> 01:19:32,650
Alright? Alright.
