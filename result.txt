Okay, so mp3, you  do three algorithms, right?  Monte-carlo, something like this.  You start somewhere.  Some poll. A trajectory starting from,  say sum is zero and goes to as T. And in there are some,  Everything is sum as i for some i.  And then for each state,  look at the trajectory,  the part of the trajectory that goes from  that state to the terminal state.  And understand that, that is  one sample of the possible trajectories  that you can think of that can  possibly happen going out from that particular state.  So you calculate  the discounted  somehow rewards.  Is this confusing?  Okay?  This is offsetting this I to zero, right?  And you count from there to the termination.  And then for each of these states,  collect a bunch of these g's, right?  And then in the end,  when we ask for the value there,  the Pi given to you as just a,  the average of that,  which is the sound of g  divided by the number, the average.  And the second algorithm,  hopefully you fully understand  now is saying it doesn't matter  where your status just  go with the flow going into the game.  Whenever you take one transition from S to S prime,  update the value estimates.  Your beginning state.  The first state in the pair of states.  Using.  The update formula that is looking at  the difference between your one sample of Bellman update.  Again, your true Bellman update should be thinking about  all the possible things that can happen  with some probabilities, right?  But now you see there's one sample which is like  the x k were talking about was the random variable.  So what do you want to do is do  things according to something  roughly similar to the Bellman update  in which you think about  this one sample of  going from this state to  attain the estimated value at the next state.  Again, this is one possible outcome  of all the possible things that may happen  here with a temporary guess  of the value on the next state.  Alright?  And then you look at the difference that this,  if this is going to be happening 100% of time,  then you should just set your true value to this, right?  But we know that is not the case.  We need to discount it somehow  was the knowledge about  how many times we have seen this particular state.  Right?  So you look at this one sample point,  look at the difference was  your current understanding about the value of  your S and then multiply with some learning rate,  which is dependent on,  say, alpha, dependent on S.  So this is our,  say, the number of visits to S.  Alright?  So in the code,  you should see that clearly.  So this is something that in principle, we should say.  If this is just trying to get one static average,  it's potentially just the number of visits to that state.  But in general, this can be  just something that's less than one.  And it allows you to tune the speed of  convergence by calling it  some sort of step size or learning rate.  Alright?  I think we have given  that precise number to you in the PA.  So this is the increment  you want to make to the previous estimate.  Everything has a pi here.  By looking at this one data point  and the difference was your current guests.  So this is the change you want to  make to your previous estimate.  Alright?  So everything here that  we talked about in these two algorithms  follows the given pi.  When you need to take a step,  you will follow the action that's given to you by the Pi.  And then we started talking about Q-learning,  which tries to give you  an algorithm that allows you to just go into the game,  go into the world,  the MDP, some state and started doing things.  Okay?  Then you need to answer two questions.  One is not addressed by our previous lecture yet.  What is the action that I should take?  Okay, so operationally, what should you do?  Yeah.  You don't need to answer yet, but I don't have  a policy here now.  So Q-learning, so no pi now,  this is our new question.  So we want to just go into,  because no one gave us others about what to do.  And let's come up with an algorithm that  is figures out the best thing to do that.  I'll feel lousy. Yeah. So you don't  start with a policy anymore.  Okay.  So we need to answer this question.  And then we partially started talking about,  in this case, what should be  the value that you want to keep track of?  The difference?  Again, is when we have a fixed pie,  we can say a state.  You follow the order  of policy given to you and it branches out, blah, blah.  And now we don't have a policy anymore.  We have a bunch of possible choices.  Alright, each one corresponds to  an action that I can possibly take.  And then we're after you  decide on taking any particular actions,  the future is potentially very different  with Neil probabilities that depend on the action.  And we were saying, for instance,  think about the state of having two cards of ten.  If I ask you,  what is the value of state?  For that state, then it's pretty hard to answer, right?  Because whether it's a good state  or a bad state depends crucially on what you actually do.  That state, if you take the bad action and  it becomes a terrible state, alright?  If you take the good action,  that is the state that you always will be happy to see.  You'll want to bet more money on that state.  Alright, so because of that,  we realized that in the case of not having  a policy and you need to form  your own understanding about actions that states.  We should start thinking about  the values of state and action pairs.  That's what we call q-value,  is no longer a state value,  but it's a state action that  then we want to do this in the same way  as what Bellman equation told us.  Because we know if we do things according  to Bellman equation and everything is contraction,  nice and converging and all that, right?  So we should try our best to make sure even  if we shift our focus to this state action pair of thing,  we shouldn't deviate from what  Bellman equation has formulated for us.  Alright?  Now, the option for that, one suggestion,  one design for doing  value estimates on state action pairs now is to think of  state values as the best Q value that you can get.  Right?  So you want to do that again.  This is exactly what you did in 2048, right?  So you are, suppose you have computers  and members for those and you're  wondering about what actions to take here,  just look at those numbers and take the max.  Then max.  If you annotate the root node or  the intermediate max player's node should be the number  you backpropagate to the upper level, right?  This is doing exactly that.  Okay?  And then all we want to do,  so this is the definition of  Q values that we want to use to  keep track of the quality of actions.  And then we want to make sure  that we are going to write down something  that's consistent with the Bellman equation because  that's the best thing ever, right?  So we know the value is,  according to Bell men.  Yeah, the expectation value  of next states given as an aid.  Then, well we can do is push  everything in to the max operator and expectation.  Because if you're maximizing something,  you can maximize it up to  some constant factor and an additional constant term.  So nothing in here  is dependent on this number or this number.  So I can say maximize a  in S. And then  same thing can be said about the expectation.  I can push everything into the expectation, right?  So all I want to do for each state is to  maximize the expectation of given us an a.  Okay?  So now you see this  becomes what I can define as the q-value.  Great.  So now let's just do that.  So the Q value of state action pair,  according to Bellman equation,  I only need it to be the expectation of  R S plus Gamma V S prime.  If you do things according to S and a.  Correct?  So basically, if I define q-values to be this,  then I can plug it in here  and recover the Bellman equation.  Right?  And again, keep in mind that this makes sense.  According to expectimax, what they were doing expectimax  is you're trying to put a number here, right?  And then number is really  the expectation of these numbers.  The expectation of what happens at the next state.  And the only difference now is I consider rewards at  every step I can take and there's a discount factor.  So the only difference between  the two antibiotics and here is you  can collect rewards that  every state and you have a discount factor.  Other than that, you're just keeping track of what  you were computing for 2048.  And now we give it a name,  call it Q values.  Okay?  And then if I want to take even one step further to just  not see the state value  at all because it is the maximization of q-values anyway.  Right?  So I can just rewrite  my whole Bellman equation with only Q values,  which will be the expectation of  R S plus gamma times what?  The maximum of a at the next state.  Correct.  And all your possible choices add the next state.  So whatever as prime I have here,  if I want to know about the number here,  I should think about that maximum action that you can  take there and then their futures, right?  So things just happen recursively down one level.  So all I need to say recursively  is I maximize the action at the next state,  and then I get what as primary prime.  Again, this is just rewriting this,  this, There's nothing tricky happening here.  And then as an a is  whatever is the probability dependent now.  Okay? Yeah, good question.  Okay, question for this part.  So first of all, this is all dependent on,  I kind of have a way of taking actions.  I'm just writing actions everywhere.  We haven't given that yet,  but there'll be given mercy.  Go ahead.  That's the question.  We do know all the actions.  The environment doesn't change for  the fixed MDP that we are dealing with.  We just don't know the values of those actions.  We need to play the game,  the game is fixed, the simulator is given to you.  And you need to somehow  understand which action is good, which action is back.  Yeah.  Okay, so now what  I'm going to say is I propose the algorithm to be,  I start from some state.  I somehow take an action.  We'll elaborate on that.  And then I see a transition to S prime.  Okay?  I have my own understanding of my Q values.  And I want to use this one transition  to update my new understanding  about this extra state action pair I took.  What would you do? So in  temporal difference learning which you  are okay now what is hopefully,  we say according to a fixed pie,  you look up the action, right?  And then we want to find out, you know,  from the ODE as prime,  I want to get we  knew right frontal view  owed on his crown and build on it.  Okay, so that's what we did  for temporal difference learning was a fixed pie.  And now what I want to do is Q as crime.  Somehow a prime and Q S and a 0, 0.  I want to use that to know.  Now what should I understand  about my particular state and action choice?  Again, the problem setup is exactly the same.  It's just that now I have this additional action, right?  And I want to go from  my previous or current old estimate  of the values to get new values.  What would you suggest that we do?  Now?  Yeah.  Yeah.  That's what we want to do. So first of all,  from the Bellman equation for the state values,  you're all k was the Bellman equation for the Q values.  Right?  Now let's just pretend you've  never heard about state values.  I'll you know, all you have are these q-values.  It's always over state action pairs.  And we came up with that temporal difference formula.  When everything is overstates,  was exactly the structure of S is the expectation  of R S plus Gamma v as  prime for a fixed pie, right?  So we had that.  And then from this we say, okay,  this is estimating an expectation.  So let's look at one day the point of  this and compare it with  your current understanding and use that to  multiply it with the learning rate  to update your understanding, right?  So you're okay with that.  Now, all that we're changing this to is  q essay is the expectation.  Again, RS plus max of a prime,  Q prime, a prime.  Okay? And in the state value case,  I wanted to update this.  And now I want to update this.  The algorithm should be very similar, Correct?  What would you do?  Yeah.  Yes. So the update rule I would  propose is Q as a new should get.  First of all, I look at  this one-step experience going from S to S prime.  After a look at this,  once that data point,  which is the thing I can collect inside this expectation,  which is what we are trying to take  running average earth, right?  I look at R of S. Plus.  The only tricky thing is this max that you are afraid of.  But really, it doesn't  matter because everything is you're saying,  oh, the Q values.  So I have them write disliking state value case.  I assume I started with just  random guessing all the states.  And for Q is I start from random guys and all the cues,  whether update or not, they're just there.  You have the matrix over essay.  Now, it used to be  just a bunch of numbers for your V of S.  Now there's a bigger table,  but you still have them starting  from all zeros aren't brand whatever you want.  Whenever you ask questions about  what is the max a prime for the next state,  you just look at whatever current guess you have.  This is something that you can always  do according to your current old value.  So all you need to do is first,  of course, lookup where your next state Q value entry is.  And then take the max of that. Again.  Whenever you view.  The max operator is annoying.  Just keeping in mind, the only reason we're writing  this is to pretend that we only have two values.  But if you actually keep as the value for your S prime,  that is also something you can just look up.  If you just have another lookup table that  always maximize whatever actions you have,  an essay that is your best to stay value.  You can just plug in that as well, okay?  But you really don't need to do  that and you just need to keep  one lookup table and every  time you need to query for the next state,  take the best possible thing.  You don't want to be pessimistic there, right?  Because we're looking for the best possible, right?  And then look at the difference between that  and my  current q-value.  Look at their difference.  And this is in principle how you should modify  your previous understanding of  this particular state action pair.  Pick this difference, multiply worse than Alpha,  which now depends on the visits to the state action pair.  Right?  Now, the state action pairs are the things that you  care about knowing the statistics off.  Okay?  So there may be some state if you  start with a reasonable enough policy,  if you're a 1010, probably in most cases, you just well,  you always stand right then  probably you would with it the pair  of plenty and stand much more often than 20 and hit.  Right? So for the same state,  you may have different visitation for different actions.  That's fine.  Whatever it is, just use  that number and turn that into a learning rate.  Okay.  And that is the increment you want to do for  your current understanding of  the cube? That's how you update.  Yeah.  Yes.  How many times you actually try  out this particular action when you see that state.  Again.  The straightforward way is really just the number  of visits to that state.  One divided by that.  Yeah. Just like one over K.  Go ahead.  Yes. So that's a very good point.  That's a we're going to spend the whole next week.  So first, every symbol we spent at least one week.  The more you try out a particular action,  the better you know about it.  The more you try out a particular state action pair,  the less this alpha is going to get,  which means the last new experience appearance  is going to matter, right?  And at the same time,  if you have a very bad choice,  you probably don't want to visit it very often,  just so that you know more about it.  If you know some option is bad enough,  you probably want to limit your visits to that,  which at the same time will also limit  your understanding about that potentially bad choice.  So if you know nothing,  there is a trade-off  between going was the best thing that you  currently see versus going  to something that's not the best that you see,  but you expect that to be potential,  meaning that maybe your few samples  in the beginning are not.  Representative enough for its true expectation.  So this is the fundamental trade-off  called exploration versus exploitation.  Exploitation means you go for the action  that for the time being you  believe to be the to be the best thing.  But if that's the only thing you do,  you will miss out on other things that you  probably just didn't know enough about, right?  So exploration will be the other side, which is,  Let's just try to maximize  our understanding about all the options.  Rather, don t think about whether they're good or bad.  Okay?  So now there's two objectives.  One is to maximize  your understanding or information about all the choices.  The other is you still want to maximize  your overall rewards, right?  You go into a casino.  Yes, you are trying to learn to play blackjack,  but also you have a fixed budget. Then what do you do?  That is the trade-off.  That will be the topic of art, mainly next chapter.  But here you see a flavor of that, right?  And that actually gives us the policy.  So when you are choosing  actions based on your Q values  that you're updating dynamically.  You want to go with the actions that  are as good as possible in the sense  that you want to get as much total rewards as possible.  And at the same time,  you want to always leave out some probabilities are  some opportunities for actions that  currently seem bad in terms of q-value.  Alright?  What, what would you propose to do?  If that's the thing you want to balance?  So that's just okay.  So this, the queue add date is fine now, alright?  So this is just whenever you somehow choose an action,  you just look at whatever Q values are  for the next date and then use that to update things.  And since this is Bellman equation,  we know if we do things  lean this way, it's going to converge.  Now, the only question left is how we choose the action?  Go ahead.  Yeah.  You mean which are the same?  These two things are these and this.  So when you do this update,  you're changing this one entry in your q table, right?  Everything is a lookup.  And the only thing you are changing is  this state action pair that you are trying out right now.  Okay?  So you have these whole q table for all your state law,  your actions there, this one cell  that you're changing was this, right?  And then the change to that  is determined by this formula,  which looks up this value  and looks up your current value for this.  And these are two very different numbers  potentially. Alright?  And then just do whatever this formula is saying.  So there's one step update only changes  one entry in your queue lookup table.  Good.  Yeah,  sure.  I have 1010.  And I know nothing about the game.  The first time I tried it, I asked for a new card.  Somehow I get a one that can  happen in my world and this particular round, right?  So from that, I understand the Q value of   I have one transition from this to tan,  tan, tan, tan, tan one.  And when? That's all I ever have seen in blackjack.  Okay.  Based on that, no one  can say My understanding of the game is wrong.  It's totally true based on my update because I go there,  this becomes a terminal state.  I get my money and I update this based  on the next maximum is the amount I'm getting, right?  So the Q value of  this particular state action pair  is positive, at least, right?  And this is definitely higher than q can stand,  which I came in with a random guess of zero.  So then what do you do?  If you encounter this date again?  You for them too?  Infinity, always ask for a new card.  You don't.  Right?  So the difficulty of exploration  versus exploitation is really crucial,  dependent on the world being stochastic.  Your sample may never,  your smart number of samples may never give you  a precise enough understanding about the transition.  But what you can hope for is if you have played  this action at this state  for a reasonable number of times,  hit is still better than stand.  It both have been played  and hit is better,  which will not happen, right?  But if that's the case,  then you basically have  statistical guarantees saying I have simple enough  so that I have reached  a stable understanding of that particular option.  And based on that, I choose the better one.  The more times you visit that action state action pair,  the more reliable your experience.  Yes.  And that is why the more wisdom so you have,  the less alpha we're going to use.  Yes, there'll be one choice.  Right? Yeah.  Stabilizes higher than your initial guess, probably.  Wouldn't you, in future classes save it,  then you would get punished for it.  So the question is,  if I just insist not trying the other option, right?  Do I still okay.  To just keep exploiting one action  and formulae in a policy like that?  Yeah.  Yes.  Let's say you do this and then you  say after ten times I get a minus as five or minus nine.  Okay.  And the question is, is that enough?  The answer is depends on what you came in.  And was that the estimate for that?  What if I came in and say everything started from -20?  I'm allowed to do that without any sample at all.  You really have no idea at all.  Good.  Yeah, partially. We need to do that.  Okay.  We're going to keep talking about that,  but other questions about for right now.  But all these discussion about the action to  take already gave us the default way of doing Q-learning,  which is called epsilon greedy policy.  And you already have the,  have the name for it,  which is every time suppose,  whenever I need to take an action,  I just look at my current q-value on  each action and go with the maximum one.  Go is the best one,  then that is what we can call greedy.  So at all times you have a queue SMA  to starting from anything  and then you gradually update them.  At all times you have that comparison to make.  And you can always take the maximum,  the best action that maximizes the Q value.  So that's called greedy.  And then at same time, we said,  don't always do that  because you don't really know what is  the statistical ground truth of all your other actions.  You may just be unlucky and happened to see a bunch  of samples that are not  representative of their true potential.  Alright?  So always leave Alice and  probabilities for other actions.  So I have some greedy is just saying with Amazon  being 020 to 0.5, Let's see.  Okay.  It can be, well,  I don't recommend setting it to be  exactly zero or exactly 0.5.  So let's just say in-between.  So with one minus epsilon,  which is the slightly higher probability play action  that maximizes Q essay.  And with epsilon probability,  try out other things.  That's it.  That's the answer of,  you know, in Q-learning,  what should I do here?  Okay?  So this is  sort of an intuitive and straightforward or naive way  of balancing the exploration versus exploitation, right?  I just say, let me just give  a fixed higher probability  to the currently better action.  And hopefully by playing that action,  I will get more rewards, right?  And then at the same time,  always leave out the opportunities  for other actions because you never know.  So assign some small probability  to always explore something else.  Now, this is good  enough because in reinforcement learning,  you don't really care about how fast you learn.  All we talked about is you do things, you know,  temporal difference and eventually  they converge the right thing.  And now we know this will converge to  the right thing because even if  we leave out a small probability for the bad actions,  if time goes to infinity,  you're always going to visit them  for enough number of times.  So the visitation to  all state action pairs is always infinite.  Because I have non-zero probability for each choice.  Then I would know this update  for every state action pair is going to  happen for a long enough time.  And we know because of contraction,  as long as you allow it to go enough times,  it will converge to the right thing.  Good for our PA and logistics it.  In reality, shrinking it.  Or actually in reality,  what you would do is you do  whatever absolute choice you  get in the Q-learning process.  And then when you actually need to run the policy,  you basically make it deterministic.  You always go for the greedy one.  So basically the probability on other things allows  you to gather more information or do more learning.  And then when you are asked to  stop and just play the blackjack,  blackjack was the best policy you have.  You would just do epsilon being zero.  And we know by doing this,  you will be able to separate the true q values,  the good actions versus the bad actions.  And if you always go with the ground,  choose best to Q value,  you will reach the optimal policy.  So by doing Q-learning and this way again,  choosing actions space down  your temporary guests of  the q-values and epsilon greedy way,  and then update your Q values,  which is updating your estate values  based on whatever current policy you have.  Eventually this whole procedure is going to  give you the best possible policy.  That's the answer to  the whole reinforced learning problem in  this setting of finite and discrete state  and finite discrete action space.  Okay?  So Q-learning is in some sense  a perfect algorithm if you don't care so  much about the running time, right?  Because it doesn't require you to  know anything about the MDP.  Well, anything about the transition function, right?  So we don't assume any knowledge about the probability.  We just say that we know is there and is fixed.  All we need to do is to sample  transitions from the MDP based on  this explicit choice of actions is  a very practical algorithm.  You can implement it. And then we know based  on Bellman equation contraction of the law.  In the end, it's always going to  converge to the right answer.  The right answer being finding  the optimal action at every state,  which means finding the optimal policy.  Okay? So that fully answers how to  play blackjack after three weeks.  Okay?  So this is the third algorithm you are supposed to  implement in that PA.  Questions about that.  So first of all, let me ask again.  What is the Q value of a terminal state?  Which doesn't quite make sense, right?  Yeah, go ahead. Yeah. Right.  Yeah, exactly.  Right.  So we talked about the state value of  the state value and our fixed policy of  a terminal state will just be  the reward of that state right?  Now. Same thing for the q values.  First of all, yes,  there's no actions to take in a terminal state.  So your Q essay is basically  just VFS when S is just the terminal.  And then every time you visit  a particular state with no action at the terminal state,  you are going to do this,  update and increment the estimate that  that q-value a little bit towards its reward, right?  So after enough time,  you're just going to converge to  the ground truth reward at the terminal state.  So if you happen to set your initial guess of  the Q values to be  exactly the reward at the terminal state.  You are definitely correct.  And we said it doesn't even matter.  Any initial guess you can use,  but that'll be a good initialization.  Right?  Other other questions.  Yeah.  Sorry, the epsilon greedy policy.  Yes.  So this is an algorithm  that needs to tell you how to play the game, right?  So when we're doing  the timber different than our fixed policy,  whenever you need to go for a step,  you just query the policy which has given to you.  And now we don't have a policy to start with anymore,  but we formulate our own policy  according to the epsilon greedy rule, right?  Yeah, So every time just looking at in the beginning,  everything is zero, for instance,  that's your neutralization.  And then just take whatever, right?  So you'd say probability of the best one,  the best one, everything is the best.  So just take an arbitrary one  and then you are going to start  accumulating interesting numbers for  Q values of all your actions.  And yeah, just, just go with that formula.  Okay, So all the pseudocode and stuff  is It's in the slides.  You know, that's, that's the code  for Q-learning and yeah,  you should be able to just implemented.  And in the PA, basically,  it first asks you to do these and then you,  once you implement Q-learning,  you can let it do Q-learning for some time.  And then there's a button called autoplay.  Autoplay just takes your current q-values  and start taking the greedy actions.  If in the earliest stage of your Q-learning,  you probably don't have  very good understanding about the actions  that as your Q-learning stabilizes,  meaning all these value estimates get  closer and closer to their ground truce, correct value.  When you hit the autoplay,  you're basically playing with  the optimal policy for this version of blackjack.  And then there's some kind of win rate that you can see.  What is the win rate that you can get  under the optimal policy that's obtained from Q-learning.  And that's how you fully solve that.  Okay? Alright. Okay.  So now again,  overall MDP and reinforce  morning you have values and policies to think about.  Everything works out.  Was these weird formulas just because,  you know, they're lying all the questions.  There's the structure of  the value space and the kind of policy space.  And you know, there's contraction  Bellman equation, blah, blah.  That happens.  And as long as you're doing things in the right way,  they will always converge  the best possible answer for all these questions.  And I don't want to start the kind of  deep reinforcement learning here  because I think we're going to come back to it.  I mean, we won't really cover  deep reinforcement learning because  it's just you saying, Well, it's different.  For asthma learning allows you to do this.  But really it's doing the same algorithm,  but throw in neural networks there.  Okay?  So in deep reinforcement learning,  basically the idea is right now in your PA,  everything is a lookup table.  But in general, you can use  a neural network to replace the lookup table.  Okay, for those of you who have done  machine learning, you know,  deep learning, you know,  that's essentially a one-year networks are doing, right?  You give it a lot of inputs,  that give it a lot of outputs and  tell it, memorize these.  And then it's going to be able to reproduce that  for one thing and also kind of  give you an interpolation on new queries.  So once you put neural networks in  the reinforced learning structure,  you can start dealing with as  much louder States and much larger action spaces.  You can even deal with  infinite continuous data and action spaces,  which you wouldn't need to do this, right?  Because you have all these  physical parameters and they are in  principle continuous values that  you cannot have a simple lookup table for.  Alright, so we're not going  to talk about that too much about later.  I will still try to come back to it.  By the way, this is just saying, you know,  your values and your policies can all be neural networks.  Alright?  Yeah, I have some slides on that,  but I want to mention this a little bit because  the real goal for me is to don't worry about these.  None of these is part of the class.  I'm sorry.  I'm okay.  I don't have the AlphaGo slide,  but I want to mention it  because in the end I want  to fully explain AlphaGo to you.  And right now we have half story,  which is reinforcement learning.  In AlphaGo, we're going to use  q values from reinforced learning to inform.  The next algorithm we're going to talk about,  which is Monte Carlo tree search.  Okay?  So after all these three weeks in AlphaGo,  this is the only thing  that you're eventually going to use.  Which is actually the only thing you would need  because playing a Go game like that,  the only question is, at every state,  what are your estimation  of the quality of different actions?  And you'd probably just go with the best one.  But reinforce learning itself is usually not  enough for problems with large enough spaces.  Because as you see,  the correct understanding of  these q-value is all dependent on your visitation, right?  So if you only train things  with reinforcement learning and then you play  a real game was really smart people against you.  If it's only coming from data.  First of all, we know the space is too large for you  to actually reliably estimate everything.  At the same time,  your adversary can lead you to a state  that is just not sin so much in your training data.  So learning itself always has  the problem of things that you haven't  seen enough times in training than in using it.  You really don't know much about it.  And because of that,  we actually need another new version  of the minimax search, which now,  given all these new understanding about probabilities,  about taking long Carlo samples,  we can actually have  a new advanced version of Monte Carlo tree search,  which will be the other part of AlphaGo.  So in the end, when we get there,  AlphaGo is simply Monte Carlo Tree Search plus q values.  We'll get there in  Okay, So now let just start the second part.  Alright.  Okay.  So coming back to the second week, there were no problem.  He's just a huge tree that we somehow come up with,  evaluation functions and all that.  And you know, if you do this for 2048 is sort of okay.  Doing this for Chez,  you can engineer things and get reasonable answers.  But for a very long time,  people didn't know how to do this for  a real game and not bill for large enough game like Go.  And go is compare it to real problems.  Go is also still small.  But if we want to scale  up all these tree search algorithms,  we need new ideas.  New ideas in AI in the past,  say 30 years, always came from.  Kinda Theta and probabilities.  So minimax algorithm itself was from 1960s.  I think that was a time when a star and  Minimax Search and alpha-beta  pruning and all that came out.  Because when people started  thinking about these classical algorithm,  that's around time when  most of the clever ideas already came out.  And then for a long time,  there's not much progress that most of the progress is in  the engineering or the value  evaluation function and blah, blah, right?  And then starting from, let's say the 80s,  we realized that learning and data are really part of  the important piece of intelligence.  And naturally people start thinking about, okay,  you have these huge trees and you didn't know how  to predict things in this big space.  What if we try sampling?  Right?  In Rufus morning sampling worked really well.  Because from the sampling,  you actually figure out algorithms that allow you to get  the optimal answers without  any knowledge about the underlying environment, right?  So then people will start thinking, Okay,  what if we try sampling in terms of tree search?  So the idea is the ground-truth tree is  always going to be large and you know,  but we can always play games,  play trajectories, which means  enrolling trajectories in this tree.  And can we somehow get  evaluation functions out of our samples of the tree?  Alright, so that's the setup.  This is your next PA,  which is a game that looks,  I mean, it sounds goal board about as much simpler game.  Again, I don't understand goal at all.  So this is just kinda come up for Connect five, I think.  And then showing is  kinda Monte-Carlo tree search working.  Basically figuring out what are  the options that are the most interesting,  most promising for your next step?  And I think I forgot which one is AI.  But when you do the algorithm,  right, it can play a pretty competitive game.  Usually I run a small competition  at the end of the quarter.  No extra credit associated with it.  I don't want you to get all competitive about that.  But you know, I usually get  some pretty interesting submissions  and we're going to play out some  of this round solutions and  see that they're going to fully  cover the whole board eventually in most cases.  Anyway.  So the idea is actually  to understand about what  happens when we take Monte Carlo's samples more.  Monte Carlo methods is a very general method.  You can't use it.  So first of all, I'm in current method is just saying,  suppose you have something that's  a ground truth that you don't really know, right?  Then you have access to IID samples of this ground truce.  Then from the samples you can actually  understand a lot about the ground truths.  For instance, the most classical example, if it is,  I'm gonna give you a very simple algorithm of calculating  pi without understanding anything  about calculus, for instance.  Okay?  So the algorithm that I propose is like this.  Use your computer or whatever to draw me this shape.  Okay?  So this part is one quarter of the circle or whatever.  And dance a square, right?  So as 11 on both sides.  And this is, and then what I'm gonna do  is draw random samples from the square, which you can do.  I just uniformly sample from the x  uniformly assembled around the y, right?  I just draw those samples.  Okay?  And then for each sample,  I can easily check whether given the x, y coordinates,  whether it's inside the,  you know, the, the pie or it's outside.  Right, just from San Jose, I can do that.  And then I look at  the ratio between the number of  red samples with the total number  of points I have sampled.  Okay?  I take Monte Carlo samples of  something and I take this average.  What would this average converge to?  It will converge to the racial of  this area over the square.  So it will converge to what?  Okay, I don't need to understand anything about,  you know, how pie is represented  as an infinite series of rule of law.  I will just do this experiment.  I be a computer scientist.  Alright, sample.  For each sample I check.  Then I collect them all.  In the end, I get this number,  I multiply it by four, I get pi.  Alright?  So this is the power of Monte Carlo methods.  Okay?  So from samples, you can understand  a lot about things that aren't inherently complicated,  which is what you're seeing in  machine learning these days as well, right?  So we don't really know what,  you know, what pictures are,  Caswell pictures of dogs.  And but if you try enough samples,  you can learn a model that  sufficiently differentiate the two.  Alright?  So there's this thing,  I don't know whether you get it.  By discounting the arrows  on this thing, you can calculate pi.  Anyway.  So we're gonna do that for our tree search.  So the main question that we have in Minimax Search  is you have the big trees  that you potentially need to grow, right?  But you can say,  let me grow the tree based on kind of my Q values,  which you kind of do Monte Carlo estimates on.  The idea is, let's just try out actions and see  whether they lead me to good path.  So I can start with  my root node and try out different actions.  Then from those, I can somehow  determine a way of choosing  actions and keep going down the tree.  And then from these Monte Carlo samples  of the different trees,  I can inform about the future growth of the tree.  Again, this is very similar to Q-learning, right?  So now that you have that idea in mind,  this is very similar,  but this is about growing the minimax tree eventually,  which doesn't really have probabilities in it.  Okay?  We're really just talking about growing  the ground choose minimax tree.  There's no probability this is not an MDP,  but we're going to use samples  to approximate the right evaluation functions.  So used to something that has probabilities in there  to eventually grow the tree in the most interesting part.  So eventually what we're looking for  is an algorithm that,  remember, we talked about  minimax trees is something huge, like that.  Whatever, right?  So it's a huge thing.  What I want to achieve with this new way of  growing the tree is to only grow the parts.  That leads to interesting games and ignore everything  else that will not be  useful for understanding my optimal actions.  Okay?  So this is the ground-truth tree.  I want to use sampling to  generate something like this  in the data structure eventually.  Okay, and unbalanced growth of  the ground-truth huge minimax tree.  And now you see why I wasn't emphasizing  so much on these vertical cutoff or horizontal  cut off because all those are  kind of '60s and '70s techniques.  Now that we have better understanding  about samples and distributions,  this is actually the best way to cut  out useful parts of the tree.  So we're going to cover an algorithm that does this.  So let's imagine the design of this algorithm.  You are going to start from the root node.  And then at least you should try out some actions, right?  So even if you just randomly sample actions,  or we're going to see samples of trajectory that goes  from that empty board to the end of the board.  So you have one trajectory down the tree, right?  And based on whether that trajectory  is winning or losing in the end,  you can sort of keep track of,  is this action a according to your current sample,  whether it's good or bad action.  Alright?  And then you can try something else and see  more samples of what  these different actions give you a different futures.  But then the core question to ask here is,  after doing this for awhile,  how do you actually know  which part of the tree you should actually grow? Right?  You only thing that you can keep track  of is you try some actions,  you sample their future path and you keep track of,  okay, auto my two samples.  This action had one win  and the other action had two wins.  Right?  Then in the future,  do I grow the,  the node that has one  when or do I grow the node that has two wings?  Of course, if you're a greedy,  you will go with the two when parked, right?  But everything is stochastic.  You really don't know whether  the current best option  is actually wasting your time for the future.  So this is why I say,  we're going to look at  this exploration versus exploitation  question in detail now.  Because it reinforced learning, we say,  as long as we leave out  some probabilities for visiting, everything is fine.  Everything is going to converge.  Contraction Boulevard, you just say that.  But now if we're doing Minimax Search,  imagine this is what you're going do.  It's going to be doing when you are really  implementing AI it as  putting in time for a playing chess,  playing goal, time actually matters.  So you have probably a fixed budget,  maybe 1,000 simulations, right?  Then, how do you try your best to grow the best part of  the tree and not waste your budgets  on things that you just have to try a little bit.  So the trade-off between exploration  versus exploitation is the most critical,  critical thing here this morning  because we don't care about it just ran forever.  It's the values that  we tried to figure out the best law Miller's floor.  But now we need to think about  carefully what the trade-off  between exploration and exploitation.  This.  Okay?  So now let me give you the simplest version of that.  I'm going to come back to that.  There's a multi-arm bandits,  but let me give you the simplest  game to think about this.  You have two coins,  while someone has two coins.  Alright?  So one coin has,  so you have two different coins, blue and red.  Let's say the ground truth of them,  you don t know, but let's just give that to you.  For one coin, you get 0.8 of one side.  And for the other coin.  These are not fair coins apparently.  But they both have a distribution.   And let's say you're only allowed to bet on pad,  only had gives you $1.  Right?  This is the, this is what the dealer knows.  This is not something that you know.  Okay.  I'll, you can see is these two coins.  And each time it's round,  all that you can choose is,  which coin do you put your money on?  Again, your money is only put on this, right?  So I say, I write two mu is basically if you choose this,  then you have 80% of chance or you have  expectation, right?  Or you have 0.2 being your mutate.  Every time the only choice,  the only decision you make is you  choose the blue coin or do you choose the red coin?  Again, without knowing this information.  So this is in general called  the multi-arm bandit problem.  And someone came out,  came out with their room with the name.  Basically says, imagine you go to  the casino again and now you're  interested in the slot machines.  Okay?  So Islam machine that has  a fixed distribution that you don't know about.  The machines now the the the casino knows, right?  And you have fixed amount of money  and you can just add every round,  choose one of the machines to play, right?  Suppose you only have two machines.  One is good, one is bad,  but you don't know about that.  Then how do you schedule your place?  Right?  So again, it's the core abstraction  of the question that exploration versus exploitation.  Maybe you have chosen this once.  Your first choice is the red coin, Frances Wright.  And there's 20% of trans,  you actually get money from it.  So maybe you, the first step,  you see rare coin.  And it's good.  Right?  Then question is, what do you do next?  If you're greedy, you're going to play this again, right?  And if that still is,  well, then you're going to keep playing that.  Okay?  So that's greedy.  Or if you just play them kind of in a balanced way.  Let's say you're a conservative person.  You played this once and then you play the other.  And then you play this once,  and then you play the other.  You can allocate 50% of chance on both coins.  And then in the end,  you're gonna get something, right?  Or there's the epsilon greedy strategy.  What would that be in this case?  Well, first give me something,  you know, how, how would you play this?  Let's not get to epsilon greedy yet.  How would you play it? By the way,  when I say coins here, really in the end,  they're going to be mapped to choices I put on the tree.  But for now, let's just say quiz.  Go ahead.  All right.  So let's say the red  one fairly give them a number of times.  And then the blue  one fairly give them a number of choices.  And then what maximize, right?  So then the, you know,  whatever that maximize this red or blue.  In the future.  This is a strategy called Explore than commit.  Okay?  So you first explore all your options for  a fixed amount of times.  And then from that,  you believed that you have  acquired enough information about the  two so that you can make  the comparison and go with the better one.  From there.  Sounds like this is better than this or this.  Precisely why we say this is better,  requires a bunch of formulas.  We won't talk about it, but you're going to enjoy  seeing those things coming in next week.  Okay?  So then eventually, this is not the optimal,  this is not the optimal strategy.  By the way, we're gonna give you  something that's more optimal than this provably.  And that's what you need to do a Monte Carlo tree search.  Alright? Alright.  